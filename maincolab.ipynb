{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maincolab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "fqOEaAo9jxeV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQkFF4i2jiJW",
        "outputId": "fa556c30-9ddd-4a17-93ff-bb65038e54fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount my Google Drive directory and access the training data located there\n",
        "gdrive_dir = '/content/gdrive/'\n",
        "data_dir = os.path.join(gdrive_dir, \"'My Drive'\",\"'Colab Notebooks'\",\"TransE-PyTorch-Drive\")\n",
        "\n",
        "drive.mount(gdrive_dir, force_remount=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $data_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5MdvQe6j--Z",
        "outputId": "cd102564-91ea-4c50-b9f0-2ad5efc6ca05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.py      images\t      metric.py       README.md\t\tsynth_data\n",
            "fb15k\t     maincolab.ipynb  metric_test.py  requirements.txt\n",
            "fb15k-small  main.py\t      model.py\t      storage.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_dir)\n",
        "data_dir_contents = data_dir + '/.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3W_neC4krq7",
        "outputId": "de11bc1b-dfe2-469e-b200-6cbf3a61e2b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/'My Drive'/'Colab Notebooks'/TransE-PyTorch-Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r $data_dir_contents ./"
      ],
      "metadata": {
        "id": "tt3wRfofkLyI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pJyAoLLkk00N",
        "outputId": "f4adf681-1341-4fc0-adc2-de9770058414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==0.8.1\n",
            "  Downloading absl-py-0.8.1.tar.gz (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting numpy==1.16.0\n",
            "  Downloading numpy-1.16.0-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 19.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3 MB 51 kB/s \n",
            "\u001b[?25hCollecting torch==1.3.0\n",
            "  Downloading torch-1.3.0-cp37-cp37m-manylinux1_x86_64.whl (773.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 773.1 MB 13 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.4.1\n",
            "  Downloading torchvision-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 18.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py==0.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (1.13.3)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (1.43.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (3.17.3)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 31.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (0.5.3)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.4.1->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 3)) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 3)) (4.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 3)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->-r requirements.txt (line 3)) (3.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.14.0->-r requirements.txt (line 3)) (1.5.2)\n",
            "Building wheels for collected packages: absl-py\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.8.1-py3-none-any.whl size=121174 sha256=5d13f07e043daeb6a455bb5da8bb984b56dc8e0290e6d5ad807e639670e1db25\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/91/e3/0fced4f5fbc0a051a5667096826186c9ff60f2d0e9bf0f1cdc\n",
            "Successfully built absl-py\n",
            "Installing collected packages: numpy, absl-py, torch, tensorflow-estimator, tensorboard, keras-applications, torchvision, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.0.0\n",
            "    Uninstalling absl-py-1.0.0:\n",
            "      Successfully uninstalled absl-py-1.0.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 0.18.2 requires numpy>=1.17, but you have numpy 1.16.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.3.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.3.0 which is incompatible.\n",
            "tensorflow-metadata 1.6.0 requires absl-py<2.0.0,>=0.9, but you have absl-py 0.8.1 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.0 which is incompatible.\n",
            "scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.16.0 which is incompatible.\n",
            "pywavelets 1.2.0 requires numpy>=1.17.3, but you have numpy 1.16.0 which is incompatible.\n",
            "pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.0 which is incompatible.\n",
            "pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.0 which is incompatible.\n",
            "pandas 1.3.5 requires numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\", but you have numpy 1.16.0 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.0 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.14.0 which is incompatible.\n",
            "jaxlib 0.1.71+cuda111 requires numpy>=1.18, but you have numpy 1.16.0 which is incompatible.\n",
            "jax 0.2.25 requires numpy>=1.18, but you have numpy 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cupy-cuda111 9.4.0 requires numpy<1.24,>=1.17, but you have numpy 1.16.0 which is incompatible.\n",
            "astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.0 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.8.1 keras-applications-1.0.8 numpy-1.16.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 torch-1.3.0 torchvision-0.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from absl import app\n",
        "#from absl import flags\n",
        "import data\n",
        "import metric\n",
        "import model as model_definition\n",
        "import os\n",
        "import storage\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils import data as torch_data\n",
        "from torch.utils import tensorboard\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "I3oyTmBNlBzF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.01\n",
        "seed=1234\n",
        "batch_size=128\n",
        "validation_batch_size=64\n",
        "vector_length=50\n",
        "margin=1.0\n",
        "norm=1\n",
        "epochs=10000\n",
        "dataset_path=\"./fb15k\"\n",
        "use_gpu=True\n",
        "validation_freq=10\n",
        "checkpoint_path=\"\"\n",
        "tensorboard_log_dir=\"./runs\""
      ],
      "metadata": {
        "id": "OBmmc5CqlOXi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HITS_AT_1_SCORE = float\n",
        "HITS_AT_3_SCORE = float\n",
        "HITS_AT_10_SCORE = float\n",
        "MRR_SCORE = float\n",
        "METRICS = Tuple[HITS_AT_1_SCORE, HITS_AT_3_SCORE, HITS_AT_10_SCORE, MRR_SCORE]"
      ],
      "metadata": {
        "id": "9s41goJhlHAC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm"
      ],
      "metadata": {
        "id": "tohfzm47oEk5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model: torch.nn.Module, data_generator: torch_data.DataLoader, entities_count: int,\n",
        "         summary_writer: tensorboard.SummaryWriter, device: torch.device, epoch_id: int, metric_suffix: str,\n",
        "         ) -> METRICS:\n",
        "    examples_count = 0.0\n",
        "    hits_at_1 = 0.0\n",
        "    hits_at_3 = 0.0\n",
        "    hits_at_10 = 0.0\n",
        "    mrr = 0.0\n",
        "\n",
        "    entity_ids = torch.arange(end=entities_count, device=device).unsqueeze(0)\n",
        "    for head, relation, tail in data_generator:\n",
        "        current_batch_size = head.size()[0]\n",
        "\n",
        "        head, relation, tail = head.to(device), relation.to(device), tail.to(device)\n",
        "        all_entities = entity_ids.repeat(current_batch_size, 1)\n",
        "        heads = head.reshape(-1, 1).repeat(1, all_entities.size()[1])\n",
        "        relations = relation.reshape(-1, 1).repeat(1, all_entities.size()[1])\n",
        "        tails = tail.reshape(-1, 1).repeat(1, all_entities.size()[1])\n",
        "\n",
        "        # Check all possible tails\n",
        "        triplets = torch.stack((heads, relations, all_entities), dim=2).reshape(-1, 3)\n",
        "        tails_predictions = model.predict(triplets).reshape(current_batch_size, -1)\n",
        "        # Check all possible heads\n",
        "        triplets = torch.stack((all_entities, relations, tails), dim=2).reshape(-1, 3)\n",
        "        heads_predictions = model.predict(triplets).reshape(current_batch_size, -1)\n",
        "\n",
        "        # Concat predictions\n",
        "        predictions = torch.cat((tails_predictions, heads_predictions), dim=0)\n",
        "        ground_truth_entity_id = torch.cat((tail.reshape(-1, 1), head.reshape(-1, 1)))\n",
        "\n",
        "        hits_at_1 += metric.hit_at_k(predictions, ground_truth_entity_id, device=device, k=1)\n",
        "        hits_at_3 += metric.hit_at_k(predictions, ground_truth_entity_id, device=device, k=3)\n",
        "        hits_at_10 += metric.hit_at_k(predictions, ground_truth_entity_id, device=device, k=10)\n",
        "        mrr += metric.mrr(predictions, ground_truth_entity_id)\n",
        "\n",
        "        examples_count += predictions.size()[0]\n",
        "\n",
        "    hits_at_1_score = hits_at_1 / examples_count * 100\n",
        "    hits_at_3_score = hits_at_3 / examples_count * 100\n",
        "    hits_at_10_score = hits_at_10 / examples_count * 100\n",
        "    mrr_score = mrr / examples_count * 100\n",
        "    # summary_writer.add_scalar('Metrics/Hits_1/' + metric_suffix, hits_at_1_score, global_step=epoch_id)\n",
        "    # summary_writer.add_scalar('Metrics/Hits_3/' + metric_suffix, hits_at_3_score, global_step=epoch_id)\n",
        "    # summary_writer.add_scalar('Metrics/Hits_10/' + metric_suffix, hits_at_10_score, global_step=epoch_id)\n",
        "    # summary_writer.add_scalar('Metrics/MRR/' + metric_suffix, mrr_score, global_step=epoch_id)\n",
        "\n",
        "    return hits_at_1_score, hits_at_3_score, hits_at_10_score, mrr_score"
      ],
      "metadata": {
        "id": "m854XjRPltss"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "mkRcZuJvluw_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = dataset_path\n",
        "train_path = os.path.join(path, \"train.txt\")\n",
        "validation_path = os.path.join(path, \"valid.txt\")\n",
        "test_path = os.path.join(path, \"test.txt\")\n"
      ],
      "metadata": {
        "id": "YSCbel1FmhUw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity2id, relation2id = data.create_mappings(train_path)\n",
        "\n",
        "batch_size = batch_size\n",
        "vector_length = vector_length\n",
        "margin = margin\n",
        "norm = norm\n",
        "learning_rate = lr\n",
        "epochs = epochs\n",
        "device = torch.device('cuda') if use_gpu else torch.device('cpu')"
      ],
      "metadata": {
        "id": "24SmPYo9mkTA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = data.FB15KDataset(train_path, entity2id, relation2id)\n",
        "train_generator = torch_data.DataLoader(train_set, batch_size=batch_size)\n",
        "validation_set = data.FB15KDataset(validation_path, entity2id, relation2id)\n",
        "validation_generator = torch_data.DataLoader(validation_set, batch_size=validation_batch_size)\n",
        "test_set = data.FB15KDataset(test_path, entity2id, relation2id)\n",
        "test_generator = torch_data.DataLoader(test_set, batch_size=validation_batch_size)"
      ],
      "metadata": {
        "id": "hPEYZ9Ram1B3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_definition.TransE(entity_count=len(entity2id), relation_count=len(relation2id), dim=vector_length,\n",
        "                                    margin=margin,\n",
        "                                    device=device, norm=norm)  # type: torch.nn.Module\n",
        "model = model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "pH048pJVm8Fg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_writer = tensorboard.SummaryWriter(log_dir=tensorboard_log_dir)\n",
        "start_epoch_id = 1\n",
        "step = 0\n",
        "best_score = 0.0\n",
        "\n",
        "if checkpoint_path:\n",
        "  start_epoch_id, step, best_score = storage.load_checkpoint(checkpoint_path, model, optimizer)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1OfUVaCnC5f",
        "outputId": "7df76d4c-d09f-423a-eac7-6082a8ddcb18"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransE(\n",
            "  (entities_emb): Embedding(14506, 50, padding_idx=14505)\n",
            "  (relations_emb): Embedding(238, 50, padding_idx=237)\n",
            "  (criterion): MarginRankingLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "losses = []\n",
        "test_scores = []\n",
        "for epoch_id in range(start_epoch_id, epochs + 1):\n",
        "    print(\"Starting epoch: \", epoch_id)\n",
        "    loss_impacting_samples_count = 0\n",
        "    samples_count = 0\n",
        "    model.train()\n",
        "\n",
        "    # pbar = tqdm(train_generator)\n",
        "    total_loss = 0\n",
        "    # local_step = 0\n",
        "    for local_heads, local_relations, local_tails in train_generator:\n",
        "        # local_heads, local_relations, local_tails = batch\n",
        "        local_heads, local_relations, local_tails = (local_heads.to(device), local_relations.to(device),\n",
        "                                                      local_tails.to(device))\n",
        "\n",
        "        positive_triples = torch.stack((local_heads, local_relations, local_tails), dim=1)\n",
        "\n",
        "        # Preparing negatives.\n",
        "        # Generate binary tensor to replace either head or tail. 1 means replace head, 0 means replace tail.\n",
        "        head_or_tail = torch.randint(high=2, size=local_heads.size(), device=device)\n",
        "        random_entities = torch.randint(high=len(entity2id), size=local_heads.size(), device=device)\n",
        "        broken_heads = torch.where(head_or_tail == 1, random_entities, local_heads)\n",
        "        broken_tails = torch.where(head_or_tail == 0, random_entities, local_tails)\n",
        "        negative_triples = torch.stack((broken_heads, local_relations, broken_tails), dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss, pd, nd = model(positive_triples, negative_triples)\n",
        "        loss.mean().backward()\n",
        "\n",
        "        # summary_writer.add_scalar('Loss/train', loss.mean().data.cpu().numpy(), global_step=step)\n",
        "        # summary_writer.add_scalar('Distance/positive', pd.sum().data.cpu().numpy(), global_step=step)\n",
        "        # summary_writer.add_scalar('Distance/negative', nd.sum().data.cpu().numpy(), global_step=step)\n",
        "\n",
        "        loss = loss.data.cpu()\n",
        "        loss_impacting_samples_count += loss.nonzero().size()[0]\n",
        "        samples_count += loss.size()[0]\n",
        "\n",
        "        optimizer.step()\n",
        "        step += 1\n",
        "        total_loss += loss.mean()\n",
        "        # pbar.set_description('Avg Loss: {:>5,}'.format(total_loss / (local_step + 1)))\n",
        "\n",
        "    losses.append(total_loss / len(train_generator))\n",
        "    print('Avg Loss: {:>5,}'.format(total_loss / len(train_generator)))\n",
        "\n",
        "    # summary_writer.add_scalar('Metrics/loss_impacting_samples', loss_impacting_samples_count / samples_count * 100,\n",
        "    #                           global_step=epoch_id)\n",
        "\n",
        "    if epoch_id % validation_freq == 0:\n",
        "        model.eval()\n",
        "        _, _, hits_at_10, _ = test(model=model, data_generator=validation_generator,\n",
        "                                    entities_count=len(entity2id),\n",
        "                                    device=device, summary_writer=summary_writer,\n",
        "                                    epoch_id=epoch_id, metric_suffix=\"val\")\n",
        "        score = hits_at_10\n",
        "        test_scores.append(score)\n",
        "        print('Score: {:>5,}'.format(score))\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            storage.save_checkpoint(model, optimizer, epoch_id, step, best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zt2NOWminMhY",
        "outputId": "24d476d5-104c-4889-fa50-8689b6069a93"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch:  1\n",
            "Avg Loss: 1.0543805360794067\n",
            "Starting epoch:  2\n",
            "Avg Loss: 1.0105862617492676\n",
            "Starting epoch:  3\n",
            "Avg Loss: 0.9798903465270996\n",
            "Starting epoch:  4\n",
            "Avg Loss: 0.9526659846305847\n",
            "Starting epoch:  5\n",
            "Avg Loss: 0.9328758716583252\n",
            "Starting epoch:  6\n",
            "Avg Loss: 0.9120991826057434\n",
            "Starting epoch:  7\n",
            "Avg Loss: 0.8973413705825806\n",
            "Starting epoch:  8\n",
            "Avg Loss: 0.87987220287323\n",
            "Starting epoch:  9\n",
            "Avg Loss: 0.8632854223251343\n",
            "Starting epoch:  10\n",
            "Avg Loss: 0.853204607963562\n",
            "Score: 5.332192757342458\n",
            "Starting epoch:  11\n",
            "Avg Loss: 0.8391176462173462\n",
            "Starting epoch:  12\n",
            "Avg Loss: 0.8265623450279236\n",
            "Starting epoch:  13\n",
            "Avg Loss: 0.8137883543968201\n",
            "Starting epoch:  14\n",
            "Avg Loss: 0.801986813545227\n",
            "Starting epoch:  15\n",
            "Avg Loss: 0.7888036370277405\n",
            "Starting epoch:  16\n",
            "Avg Loss: 0.7784390449523926\n",
            "Starting epoch:  17\n",
            "Avg Loss: 0.7689791917800903\n",
            "Starting epoch:  18\n",
            "Avg Loss: 0.7588694095611572\n",
            "Starting epoch:  19\n",
            "Avg Loss: 0.7458746433258057\n",
            "Starting epoch:  20\n",
            "Avg Loss: 0.7399624586105347\n",
            "Score: 8.140861134873111\n",
            "Starting epoch:  21\n",
            "Avg Loss: 0.7314562797546387\n",
            "Starting epoch:  22\n",
            "Avg Loss: 0.718636155128479\n",
            "Starting epoch:  23\n",
            "Avg Loss: 0.7097998857498169\n",
            "Starting epoch:  24\n",
            "Avg Loss: 0.7041687369346619\n",
            "Starting epoch:  25\n",
            "Avg Loss: 0.6938599944114685\n",
            "Starting epoch:  26\n",
            "Avg Loss: 0.6841662526130676\n",
            "Starting epoch:  27\n",
            "Avg Loss: 0.6770728826522827\n",
            "Starting epoch:  28\n",
            "Avg Loss: 0.6669766902923584\n",
            "Starting epoch:  29\n",
            "Avg Loss: 0.6620904803276062\n",
            "Starting epoch:  30\n",
            "Avg Loss: 0.6528176069259644\n",
            "Score: 9.98289136013687\n",
            "Starting epoch:  31\n",
            "Avg Loss: 0.6470375061035156\n",
            "Starting epoch:  32\n",
            "Avg Loss: 0.6393092274665833\n",
            "Starting epoch:  33\n",
            "Avg Loss: 0.6319712996482849\n",
            "Starting epoch:  34\n",
            "Avg Loss: 0.623988151550293\n",
            "Starting epoch:  35\n",
            "Avg Loss: 0.6162968873977661\n",
            "Starting epoch:  36\n",
            "Avg Loss: 0.609604001045227\n",
            "Starting epoch:  37\n",
            "Avg Loss: 0.6022545099258423\n",
            "Starting epoch:  38\n",
            "Avg Loss: 0.5965641140937805\n",
            "Starting epoch:  39\n",
            "Avg Loss: 0.5892990827560425\n",
            "Starting epoch:  40\n",
            "Avg Loss: 0.5818718671798706\n",
            "Score: 11.169090390647277\n",
            "Starting epoch:  41\n",
            "Avg Loss: 0.5744935274124146\n",
            "Starting epoch:  42\n",
            "Avg Loss: 0.5714405179023743\n",
            "Starting epoch:  43\n",
            "Avg Loss: 0.5639159083366394\n",
            "Starting epoch:  44\n",
            "Avg Loss: 0.5578796863555908\n",
            "Starting epoch:  45\n",
            "Avg Loss: 0.5510463714599609\n",
            "Starting epoch:  46\n",
            "Avg Loss: 0.5467638969421387\n",
            "Starting epoch:  47\n",
            "Avg Loss: 0.5399602651596069\n",
            "Starting epoch:  48\n",
            "Avg Loss: 0.5308765172958374\n",
            "Starting epoch:  49\n",
            "Avg Loss: 0.5291643738746643\n",
            "Starting epoch:  50\n",
            "Avg Loss: 0.5223042964935303\n",
            "Score: 12.149985742800114\n",
            "Starting epoch:  51\n",
            "Avg Loss: 0.5162379145622253\n",
            "Starting epoch:  52\n",
            "Avg Loss: 0.5101339817047119\n",
            "Starting epoch:  53\n",
            "Avg Loss: 0.5053367614746094\n",
            "Starting epoch:  54\n",
            "Avg Loss: 0.4995465576648712\n",
            "Starting epoch:  55\n",
            "Avg Loss: 0.4937470853328705\n",
            "Starting epoch:  56\n",
            "Avg Loss: 0.49025675654411316\n",
            "Starting epoch:  57\n",
            "Avg Loss: 0.4826282262802124\n",
            "Starting epoch:  58\n",
            "Avg Loss: 0.47920098900794983\n",
            "Starting epoch:  59\n",
            "Avg Loss: 0.47325387597084045\n",
            "Starting epoch:  60\n",
            "Avg Loss: 0.4685663878917694\n",
            "Score: 12.720273738237811\n",
            "Starting epoch:  61\n",
            "Avg Loss: 0.46521055698394775\n",
            "Starting epoch:  62\n",
            "Avg Loss: 0.4576580226421356\n",
            "Starting epoch:  63\n",
            "Avg Loss: 0.4553186893463135\n",
            "Starting epoch:  64\n",
            "Avg Loss: 0.4495164155960083\n",
            "Starting epoch:  65\n",
            "Avg Loss: 0.44489026069641113\n",
            "Starting epoch:  66\n",
            "Avg Loss: 0.438881516456604\n",
            "Starting epoch:  67\n",
            "Avg Loss: 0.4336630403995514\n",
            "Starting epoch:  68\n",
            "Avg Loss: 0.4295501708984375\n",
            "Starting epoch:  69\n",
            "Avg Loss: 0.4246683716773987\n",
            "Starting epoch:  70\n",
            "Avg Loss: 0.42141038179397583\n",
            "Score: 13.333333333333334\n",
            "Starting epoch:  71\n",
            "Avg Loss: 0.41831734776496887\n",
            "Starting epoch:  72\n",
            "Avg Loss: 0.41304099559783936\n",
            "Starting epoch:  73\n",
            "Avg Loss: 0.4090040326118469\n",
            "Starting epoch:  74\n",
            "Avg Loss: 0.40395960211753845\n",
            "Starting epoch:  75\n",
            "Avg Loss: 0.4001728594303131\n",
            "Starting epoch:  76\n",
            "Avg Loss: 0.3951703906059265\n",
            "Starting epoch:  77\n",
            "Avg Loss: 0.39159920811653137\n",
            "Starting epoch:  78\n",
            "Avg Loss: 0.3880605697631836\n",
            "Starting epoch:  79\n",
            "Avg Loss: 0.38236674666404724\n",
            "Starting epoch:  80\n",
            "Avg Loss: 0.38058722019195557\n",
            "Score: 13.741089250071287\n",
            "Starting epoch:  81\n",
            "Avg Loss: 0.37520739436149597\n",
            "Starting epoch:  82\n",
            "Avg Loss: 0.36908426880836487\n",
            "Starting epoch:  83\n",
            "Avg Loss: 0.3694795072078705\n",
            "Starting epoch:  84\n",
            "Avg Loss: 0.3629654049873352\n",
            "Starting epoch:  85\n",
            "Avg Loss: 0.35987138748168945\n",
            "Starting epoch:  86\n",
            "Avg Loss: 0.3564782440662384\n",
            "Starting epoch:  87\n",
            "Avg Loss: 0.3535196781158447\n",
            "Starting epoch:  88\n",
            "Avg Loss: 0.3492030203342438\n",
            "Starting epoch:  89\n",
            "Avg Loss: 0.34666332602500916\n",
            "Starting epoch:  90\n",
            "Avg Loss: 0.3426353931427002\n",
            "Score: 14.265754205873964\n",
            "Starting epoch:  91\n",
            "Avg Loss: 0.3380579352378845\n",
            "Starting epoch:  92\n",
            "Avg Loss: 0.3359088599681854\n",
            "Starting epoch:  93\n",
            "Avg Loss: 0.3326837420463562\n",
            "Starting epoch:  94\n",
            "Avg Loss: 0.32763242721557617\n",
            "Starting epoch:  95\n",
            "Avg Loss: 0.3251723647117615\n",
            "Starting epoch:  96\n",
            "Avg Loss: 0.32311317324638367\n",
            "Starting epoch:  97\n",
            "Avg Loss: 0.3190109431743622\n",
            "Starting epoch:  98\n",
            "Avg Loss: 0.3165873885154724\n",
            "Starting epoch:  99\n",
            "Avg Loss: 0.31197595596313477\n",
            "Starting epoch:  100\n",
            "Avg Loss: 0.30819621682167053\n",
            "Score: 14.773310521813515\n",
            "Starting epoch:  101\n",
            "Avg Loss: 0.30586469173431396\n",
            "Starting epoch:  102\n",
            "Avg Loss: 0.3031354546546936\n",
            "Starting epoch:  103\n",
            "Avg Loss: 0.3016810119152069\n",
            "Starting epoch:  104\n",
            "Avg Loss: 0.29599112272262573\n",
            "Starting epoch:  105\n",
            "Avg Loss: 0.29509246349334717\n",
            "Starting epoch:  106\n",
            "Avg Loss: 0.2918510138988495\n",
            "Starting epoch:  107\n",
            "Avg Loss: 0.28936630487442017\n",
            "Starting epoch:  108\n",
            "Avg Loss: 0.28590354323387146\n",
            "Starting epoch:  109\n",
            "Avg Loss: 0.28326186537742615\n",
            "Starting epoch:  110\n",
            "Avg Loss: 0.2815968096256256\n",
            "Score: 15.192472198460221\n",
            "Starting epoch:  111\n",
            "Avg Loss: 0.2773180902004242\n",
            "Starting epoch:  112\n",
            "Avg Loss: 0.27626678347587585\n",
            "Starting epoch:  113\n",
            "Avg Loss: 0.27408888936042786\n",
            "Starting epoch:  114\n",
            "Avg Loss: 0.27057862281799316\n",
            "Starting epoch:  115\n",
            "Avg Loss: 0.2665749192237854\n",
            "Starting epoch:  116\n",
            "Avg Loss: 0.26581287384033203\n",
            "Starting epoch:  117\n",
            "Avg Loss: 0.2642102837562561\n",
            "Starting epoch:  118\n",
            "Avg Loss: 0.26102936267852783\n",
            "Starting epoch:  119\n",
            "Avg Loss: 0.2582317292690277\n",
            "Starting epoch:  120\n",
            "Avg Loss: 0.2566344439983368\n",
            "Score: 15.548902195608783\n",
            "Starting epoch:  121\n",
            "Avg Loss: 0.2539350688457489\n",
            "Starting epoch:  122\n",
            "Avg Loss: 0.25166046619415283\n",
            "Starting epoch:  123\n",
            "Avg Loss: 0.24755620956420898\n",
            "Starting epoch:  124\n",
            "Avg Loss: 0.24873086810112\n",
            "Starting epoch:  125\n",
            "Avg Loss: 0.24530790746212006\n",
            "Starting epoch:  126\n",
            "Avg Loss: 0.24188436567783356\n",
            "Starting epoch:  127\n",
            "Avg Loss: 0.2404394894838333\n",
            "Starting epoch:  128\n",
            "Avg Loss: 0.23936480283737183\n",
            "Starting epoch:  129\n",
            "Avg Loss: 0.23713208734989166\n",
            "Starting epoch:  130\n",
            "Avg Loss: 0.2340412437915802\n",
            "Score: 15.859709153122326\n",
            "Starting epoch:  131\n",
            "Avg Loss: 0.23233196139335632\n",
            "Starting epoch:  132\n",
            "Avg Loss: 0.2320486456155777\n",
            "Starting epoch:  133\n",
            "Avg Loss: 0.22962741553783417\n",
            "Starting epoch:  134\n",
            "Avg Loss: 0.22636035084724426\n",
            "Starting epoch:  135\n",
            "Avg Loss: 0.22606271505355835\n",
            "Starting epoch:  136\n",
            "Avg Loss: 0.22353745996952057\n",
            "Starting epoch:  137\n",
            "Avg Loss: 0.22158557176589966\n",
            "Starting epoch:  138\n",
            "Avg Loss: 0.22087857127189636\n",
            "Starting epoch:  139\n",
            "Avg Loss: 0.21697962284088135\n",
            "Starting epoch:  140\n",
            "Avg Loss: 0.21662531793117523\n",
            "Score: 16.256059309951524\n",
            "Starting epoch:  141\n",
            "Avg Loss: 0.2147349864244461\n",
            "Starting epoch:  142\n",
            "Avg Loss: 0.2125939577817917\n",
            "Starting epoch:  143\n",
            "Avg Loss: 0.21086625754833221\n",
            "Starting epoch:  144\n",
            "Avg Loss: 0.20924057066440582\n",
            "Starting epoch:  145\n",
            "Avg Loss: 0.20809927582740784\n",
            "Starting epoch:  146\n",
            "Avg Loss: 0.20489637553691864\n",
            "Starting epoch:  147\n",
            "Avg Loss: 0.20530012249946594\n",
            "Starting epoch:  148\n",
            "Avg Loss: 0.20284906029701233\n",
            "Starting epoch:  149\n",
            "Avg Loss: 0.200445756316185\n",
            "Starting epoch:  150\n",
            "Avg Loss: 0.20012587308883667\n",
            "Score: 16.518391787852867\n",
            "Starting epoch:  151\n",
            "Avg Loss: 0.19853618741035461\n",
            "Starting epoch:  152\n",
            "Avg Loss: 0.1967516839504242\n",
            "Starting epoch:  153\n",
            "Avg Loss: 0.1956571340560913\n",
            "Starting epoch:  154\n",
            "Avg Loss: 0.1944556087255478\n",
            "Starting epoch:  155\n",
            "Avg Loss: 0.1918734908103943\n",
            "Starting epoch:  156\n",
            "Avg Loss: 0.1922987699508667\n",
            "Starting epoch:  157\n",
            "Avg Loss: 0.18979308009147644\n",
            "Starting epoch:  158\n",
            "Avg Loss: 0.18767550587654114\n",
            "Starting epoch:  159\n",
            "Avg Loss: 0.18765120208263397\n",
            "Starting epoch:  160\n",
            "Avg Loss: 0.1859865039587021\n",
            "Score: 16.79498146564015\n",
            "Starting epoch:  161\n",
            "Avg Loss: 0.18517480790615082\n",
            "Starting epoch:  162\n",
            "Avg Loss: 0.1816682517528534\n",
            "Starting epoch:  163\n",
            "Avg Loss: 0.18204626441001892\n",
            "Starting epoch:  164\n",
            "Avg Loss: 0.1799883246421814\n",
            "Starting epoch:  165\n",
            "Avg Loss: 0.1782517284154892\n",
            "Starting epoch:  166\n",
            "Avg Loss: 0.17889191210269928\n",
            "Starting epoch:  167\n",
            "Avg Loss: 0.1771823763847351\n",
            "Starting epoch:  168\n",
            "Avg Loss: 0.1769431084394455\n",
            "Starting epoch:  169\n",
            "Avg Loss: 0.1758047491312027\n",
            "Starting epoch:  170\n",
            "Avg Loss: 0.1739877462387085\n",
            "Score: 17.057313943541487\n",
            "Starting epoch:  171\n",
            "Avg Loss: 0.17252230644226074\n",
            "Starting epoch:  172\n",
            "Avg Loss: 0.17031893134117126\n",
            "Starting epoch:  173\n",
            "Avg Loss: 0.16993871331214905\n",
            "Starting epoch:  174\n",
            "Avg Loss: 0.16957302391529083\n",
            "Starting epoch:  175\n",
            "Avg Loss: 0.16760359704494476\n",
            "Starting epoch:  176\n",
            "Avg Loss: 0.16866527497768402\n",
            "Starting epoch:  177\n",
            "Avg Loss: 0.16640251874923706\n",
            "Starting epoch:  178\n",
            "Avg Loss: 0.16379939019680023\n",
            "Starting epoch:  179\n",
            "Avg Loss: 0.16332338750362396\n",
            "Starting epoch:  180\n",
            "Avg Loss: 0.16306546330451965\n",
            "Score: 17.339606501283146\n",
            "Starting epoch:  181\n",
            "Avg Loss: 0.16167324781417847\n",
            "Starting epoch:  182\n",
            "Avg Loss: 0.16083167493343353\n",
            "Starting epoch:  183\n",
            "Avg Loss: 0.15939103066921234\n",
            "Starting epoch:  184\n",
            "Avg Loss: 0.15976759791374207\n",
            "Starting epoch:  185\n",
            "Avg Loss: 0.15777777135372162\n",
            "Starting epoch:  186\n",
            "Avg Loss: 0.15419138967990875\n",
            "Starting epoch:  187\n",
            "Avg Loss: 0.1554507315158844\n",
            "Starting epoch:  188\n",
            "Avg Loss: 0.15463906526565552\n",
            "Starting epoch:  189\n",
            "Avg Loss: 0.15516303479671478\n",
            "Starting epoch:  190\n",
            "Avg Loss: 0.15283578634262085\n",
            "Score: 17.65041345879669\n",
            "Starting epoch:  191\n",
            "Avg Loss: 0.1522369533777237\n",
            "Starting epoch:  192\n",
            "Avg Loss: 0.15075592696666718\n",
            "Starting epoch:  193\n",
            "Avg Loss: 0.15127015113830566\n",
            "Starting epoch:  194\n",
            "Avg Loss: 0.14879578351974487\n",
            "Starting epoch:  195\n",
            "Avg Loss: 0.1486133635044098\n",
            "Starting epoch:  196\n",
            "Avg Loss: 0.14858102798461914\n",
            "Starting epoch:  197\n",
            "Avg Loss: 0.1476132720708847\n",
            "Starting epoch:  198\n",
            "Avg Loss: 0.1460697203874588\n",
            "Starting epoch:  199\n",
            "Avg Loss: 0.146084725856781\n",
            "Starting epoch:  200\n",
            "Avg Loss: 0.1448473185300827\n",
            "Score: 17.852865697177076\n",
            "Starting epoch:  201\n",
            "Avg Loss: 0.14437636733055115\n",
            "Starting epoch:  202\n",
            "Avg Loss: 0.14264394342899323\n",
            "Starting epoch:  203\n",
            "Avg Loss: 0.1430659294128418\n",
            "Starting epoch:  204\n",
            "Avg Loss: 0.14099232852458954\n",
            "Starting epoch:  205\n",
            "Avg Loss: 0.14060164988040924\n",
            "Starting epoch:  206\n",
            "Avg Loss: 0.13929124176502228\n",
            "Starting epoch:  207\n",
            "Avg Loss: 0.14083296060562134\n",
            "Starting epoch:  208\n",
            "Avg Loss: 0.13803496956825256\n",
            "Starting epoch:  209\n",
            "Avg Loss: 0.13700026273727417\n",
            "Starting epoch:  210\n",
            "Avg Loss: 0.13739058375358582\n",
            "Score: 18.08383233532934\n",
            "Starting epoch:  211\n",
            "Avg Loss: 0.13648052513599396\n",
            "Starting epoch:  212\n",
            "Avg Loss: 0.13515597581863403\n",
            "Starting epoch:  213\n",
            "Avg Loss: 0.13432973623275757\n",
            "Starting epoch:  214\n",
            "Avg Loss: 0.13280780613422394\n",
            "Starting epoch:  215\n",
            "Avg Loss: 0.13359613716602325\n",
            "Starting epoch:  216\n",
            "Avg Loss: 0.13233335316181183\n",
            "Starting epoch:  217\n",
            "Avg Loss: 0.13244570791721344\n",
            "Starting epoch:  218\n",
            "Avg Loss: 0.13258075714111328\n",
            "Starting epoch:  219\n",
            "Avg Loss: 0.13113030791282654\n",
            "Starting epoch:  220\n",
            "Avg Loss: 0.13175302743911743\n",
            "Score: 18.323353293413174\n",
            "Starting epoch:  221\n",
            "Avg Loss: 0.12971724569797516\n",
            "Starting epoch:  222\n",
            "Avg Loss: 0.1305616796016693\n",
            "Starting epoch:  223\n",
            "Avg Loss: 0.1291341781616211\n",
            "Starting epoch:  224\n",
            "Avg Loss: 0.12754391133785248\n",
            "Starting epoch:  225\n",
            "Avg Loss: 0.12737898528575897\n",
            "Starting epoch:  226\n",
            "Avg Loss: 0.1267249584197998\n",
            "Starting epoch:  227\n",
            "Avg Loss: 0.12641213834285736\n",
            "Starting epoch:  228\n",
            "Avg Loss: 0.12553352117538452\n",
            "Starting epoch:  229\n",
            "Avg Loss: 0.12579166889190674\n",
            "Starting epoch:  230\n",
            "Avg Loss: 0.1244870126247406\n",
            "Score: 18.49443969204448\n",
            "Starting epoch:  231\n",
            "Avg Loss: 0.1232418492436409\n",
            "Starting epoch:  232\n",
            "Avg Loss: 0.12363193184137344\n",
            "Starting epoch:  233\n",
            "Avg Loss: 0.1230292022228241\n",
            "Starting epoch:  234\n",
            "Avg Loss: 0.12231996655464172\n",
            "Starting epoch:  235\n",
            "Avg Loss: 0.12146846204996109\n",
            "Starting epoch:  236\n",
            "Avg Loss: 0.12158513069152832\n",
            "Starting epoch:  237\n",
            "Avg Loss: 0.12075640261173248\n",
            "Starting epoch:  238\n",
            "Avg Loss: 0.12183557450771332\n",
            "Starting epoch:  239\n",
            "Avg Loss: 0.1192592978477478\n",
            "Starting epoch:  240\n",
            "Avg Loss: 0.11863395571708679\n",
            "Score: 18.605645851154833\n",
            "Starting epoch:  241\n",
            "Avg Loss: 0.1181584820151329\n",
            "Starting epoch:  242\n",
            "Avg Loss: 0.11720343679189682\n",
            "Starting epoch:  243\n",
            "Avg Loss: 0.11823295056819916\n",
            "Starting epoch:  244\n",
            "Avg Loss: 0.11784283071756363\n",
            "Starting epoch:  245\n",
            "Avg Loss: 0.11677820980548859\n",
            "Starting epoch:  246\n",
            "Avg Loss: 0.11646685749292374\n",
            "Starting epoch:  247\n",
            "Avg Loss: 0.11592791974544525\n",
            "Starting epoch:  248\n",
            "Avg Loss: 0.11478442698717117\n",
            "Starting epoch:  249\n",
            "Avg Loss: 0.11533752083778381\n",
            "Starting epoch:  250\n",
            "Avg Loss: 0.11520719528198242\n",
            "Score: 18.793840889649275\n",
            "Starting epoch:  251\n",
            "Avg Loss: 0.11568573117256165\n",
            "Starting epoch:  252\n",
            "Avg Loss: 0.11485950648784637\n",
            "Starting epoch:  253\n",
            "Avg Loss: 0.11360888928174973\n",
            "Starting epoch:  254\n",
            "Avg Loss: 0.11296429485082626\n",
            "Starting epoch:  255\n",
            "Avg Loss: 0.1132352203130722\n",
            "Starting epoch:  256\n",
            "Avg Loss: 0.11259353160858154\n",
            "Starting epoch:  257\n",
            "Avg Loss: 0.11116527765989304\n",
            "Starting epoch:  258\n",
            "Avg Loss: 0.11065874248743057\n",
            "Starting epoch:  259\n",
            "Avg Loss: 0.10988613963127136\n",
            "Starting epoch:  260\n",
            "Avg Loss: 0.10916963219642639\n",
            "Score: 18.942115768463076\n",
            "Starting epoch:  261\n",
            "Avg Loss: 0.10932300984859467\n",
            "Starting epoch:  262\n",
            "Avg Loss: 0.10905792564153671\n",
            "Starting epoch:  263\n",
            "Avg Loss: 0.10855323076248169\n",
            "Starting epoch:  264\n",
            "Avg Loss: 0.10799778252840042\n",
            "Starting epoch:  265\n",
            "Avg Loss: 0.10746031254529953\n",
            "Starting epoch:  266\n",
            "Avg Loss: 0.10739350318908691\n",
            "Starting epoch:  267\n",
            "Avg Loss: 0.10746069997549057\n",
            "Starting epoch:  268\n",
            "Avg Loss: 0.10664230585098267\n",
            "Starting epoch:  269\n",
            "Avg Loss: 0.10625392943620682\n",
            "Starting epoch:  270\n",
            "Avg Loss: 0.10641875863075256\n",
            "Score: 19.090390647276877\n",
            "Starting epoch:  271\n",
            "Avg Loss: 0.10553159564733505\n",
            "Starting epoch:  272\n",
            "Avg Loss: 0.10526778548955917\n",
            "Starting epoch:  273\n",
            "Avg Loss: 0.10471513122320175\n",
            "Starting epoch:  274\n",
            "Avg Loss: 0.10429428517818451\n",
            "Starting epoch:  275\n",
            "Avg Loss: 0.10484548658132553\n",
            "Starting epoch:  276\n",
            "Avg Loss: 0.10490939766168594\n",
            "Starting epoch:  277\n",
            "Avg Loss: 0.10355623811483383\n",
            "Starting epoch:  278\n",
            "Avg Loss: 0.10468008369207382\n",
            "Starting epoch:  279\n",
            "Avg Loss: 0.10277078300714493\n",
            "Starting epoch:  280\n",
            "Avg Loss: 0.10256761312484741\n",
            "Score: 19.207299686341603\n",
            "Starting epoch:  281\n",
            "Avg Loss: 0.10358331352472305\n",
            "Starting epoch:  282\n",
            "Avg Loss: 0.10209042578935623\n",
            "Starting epoch:  283\n",
            "Avg Loss: 0.10154735296964645\n",
            "Starting epoch:  284\n",
            "Avg Loss: 0.10096608102321625\n",
            "Starting epoch:  285\n",
            "Avg Loss: 0.10081767290830612\n",
            "Starting epoch:  286\n",
            "Avg Loss: 0.10053045302629471\n",
            "Starting epoch:  287\n",
            "Avg Loss: 0.0999452993273735\n",
            "Starting epoch:  288\n",
            "Avg Loss: 0.0990203246474266\n",
            "Starting epoch:  289\n",
            "Avg Loss: 0.09905247390270233\n",
            "Starting epoch:  290\n",
            "Avg Loss: 0.0991722047328949\n",
            "Score: 19.327060165383518\n",
            "Starting epoch:  291\n",
            "Avg Loss: 0.09902357310056686\n",
            "Starting epoch:  292\n",
            "Avg Loss: 0.0986366793513298\n",
            "Starting epoch:  293\n",
            "Avg Loss: 0.09736356139183044\n",
            "Starting epoch:  294\n",
            "Avg Loss: 0.0985143706202507\n",
            "Starting epoch:  295\n",
            "Avg Loss: 0.09777279943227768\n",
            "Starting epoch:  296\n",
            "Avg Loss: 0.09743472188711166\n",
            "Starting epoch:  297\n",
            "Avg Loss: 0.09804888814687729\n",
            "Starting epoch:  298\n",
            "Avg Loss: 0.09667058289051056\n",
            "Starting epoch:  299\n",
            "Avg Loss: 0.09707373380661011\n",
            "Starting epoch:  300\n",
            "Avg Loss: 0.09604869782924652\n",
            "Score: 19.389791844881664\n",
            "Starting epoch:  301\n",
            "Avg Loss: 0.09538011252880096\n",
            "Starting epoch:  302\n",
            "Avg Loss: 0.09612562507390976\n",
            "Starting epoch:  303\n",
            "Avg Loss: 0.09520323574542999\n",
            "Starting epoch:  304\n",
            "Avg Loss: 0.09510166943073273\n",
            "Starting epoch:  305\n",
            "Avg Loss: 0.09451239556074142\n",
            "Starting epoch:  306\n",
            "Avg Loss: 0.094706229865551\n",
            "Starting epoch:  307\n",
            "Avg Loss: 0.0946202427148819\n",
            "Starting epoch:  308\n",
            "Avg Loss: 0.0933699682354927\n",
            "Starting epoch:  309\n",
            "Avg Loss: 0.09399206191301346\n",
            "Starting epoch:  310\n",
            "Avg Loss: 0.09312507510185242\n",
            "Score: 19.498146564014828\n",
            "Starting epoch:  311\n",
            "Avg Loss: 0.09297946095466614\n",
            "Starting epoch:  312\n",
            "Avg Loss: 0.09268391132354736\n",
            "Starting epoch:  313\n",
            "Avg Loss: 0.09197796881198883\n",
            "Starting epoch:  314\n",
            "Avg Loss: 0.09192518144845963\n",
            "Starting epoch:  315\n",
            "Avg Loss: 0.09307481348514557\n",
            "Starting epoch:  316\n",
            "Avg Loss: 0.09126370400190353\n",
            "Starting epoch:  317\n",
            "Avg Loss: 0.09129219502210617\n",
            "Starting epoch:  318\n",
            "Avg Loss: 0.09114458411931992\n",
            "Starting epoch:  319\n",
            "Avg Loss: 0.09063714742660522\n",
            "Starting epoch:  320\n",
            "Avg Loss: 0.08997204899787903\n",
            "Score: 19.677787282577704\n",
            "Starting epoch:  321\n",
            "Avg Loss: 0.09035569429397583\n",
            "Starting epoch:  322\n",
            "Avg Loss: 0.09072956442832947\n",
            "Starting epoch:  323\n",
            "Avg Loss: 0.09039410203695297\n",
            "Starting epoch:  324\n",
            "Avg Loss: 0.09085419774055481\n",
            "Starting epoch:  325\n",
            "Avg Loss: 0.08992776274681091\n",
            "Starting epoch:  326\n",
            "Avg Loss: 0.08814676105976105\n",
            "Starting epoch:  327\n",
            "Avg Loss: 0.08851882070302963\n",
            "Starting epoch:  328\n",
            "Avg Loss: 0.08886511623859406\n",
            "Starting epoch:  329\n",
            "Avg Loss: 0.08903378248214722\n",
            "Starting epoch:  330\n",
            "Avg Loss: 0.08814571052789688\n",
            "Score: 19.786142001710864\n",
            "Starting epoch:  331\n",
            "Avg Loss: 0.08854695409536362\n",
            "Starting epoch:  332\n",
            "Avg Loss: 0.08725626766681671\n",
            "Starting epoch:  333\n",
            "Avg Loss: 0.08742845803499222\n",
            "Starting epoch:  334\n",
            "Avg Loss: 0.08772436529397964\n",
            "Starting epoch:  335\n",
            "Avg Loss: 0.08660103380680084\n",
            "Starting epoch:  336\n",
            "Avg Loss: 0.08735527098178864\n",
            "Starting epoch:  337\n",
            "Avg Loss: 0.08683190494775772\n",
            "Starting epoch:  338\n",
            "Avg Loss: 0.08709139376878738\n",
            "Starting epoch:  339\n",
            "Avg Loss: 0.08678583055734634\n",
            "Starting epoch:  340\n",
            "Avg Loss: 0.08680664747953415\n",
            "Score: 19.888793840889647\n",
            "Starting epoch:  341\n",
            "Avg Loss: 0.08590126037597656\n",
            "Starting epoch:  342\n",
            "Avg Loss: 0.08541018515825272\n",
            "Starting epoch:  343\n",
            "Avg Loss: 0.08585289120674133\n",
            "Starting epoch:  344\n",
            "Avg Loss: 0.085543192923069\n",
            "Starting epoch:  345\n",
            "Avg Loss: 0.08584078401327133\n",
            "Starting epoch:  346\n",
            "Avg Loss: 0.08527521044015884\n",
            "Starting epoch:  347\n",
            "Avg Loss: 0.08425150066614151\n",
            "Starting epoch:  348\n",
            "Avg Loss: 0.08483285456895828\n",
            "Starting epoch:  349\n",
            "Avg Loss: 0.08480815589427948\n",
            "Starting epoch:  350\n",
            "Avg Loss: 0.08410347998142242\n",
            "Score: 19.900199600798405\n",
            "Starting epoch:  351\n",
            "Avg Loss: 0.08463794738054276\n",
            "Starting epoch:  352\n",
            "Avg Loss: 0.08436784893274307\n",
            "Starting epoch:  353\n",
            "Avg Loss: 0.08381733298301697\n",
            "Starting epoch:  354\n",
            "Avg Loss: 0.08350884914398193\n",
            "Starting epoch:  355\n",
            "Avg Loss: 0.08212398737668991\n",
            "Starting epoch:  356\n",
            "Avg Loss: 0.08235632628202438\n",
            "Starting epoch:  357\n",
            "Avg Loss: 0.0820276290178299\n",
            "Starting epoch:  358\n",
            "Avg Loss: 0.08232057839632034\n",
            "Starting epoch:  359\n",
            "Avg Loss: 0.08367225527763367\n",
            "Starting epoch:  360\n",
            "Avg Loss: 0.08232761174440384\n",
            "Score: 19.971485600228114\n",
            "Starting epoch:  361\n",
            "Avg Loss: 0.08231987804174423\n",
            "Starting epoch:  362\n",
            "Avg Loss: 0.08153367042541504\n",
            "Starting epoch:  363\n",
            "Avg Loss: 0.08208125084638596\n",
            "Starting epoch:  364\n",
            "Avg Loss: 0.08186910301446915\n",
            "Starting epoch:  365\n",
            "Avg Loss: 0.08016816526651382\n",
            "Starting epoch:  366\n",
            "Avg Loss: 0.08047465234994888\n",
            "Starting epoch:  367\n",
            "Avg Loss: 0.08083029836416245\n",
            "Starting epoch:  368\n",
            "Avg Loss: 0.08067631721496582\n",
            "Starting epoch:  369\n",
            "Avg Loss: 0.08012040704488754\n",
            "Starting epoch:  370\n",
            "Avg Loss: 0.07950901985168457\n",
            "Score: 20.08554319931565\n",
            "Starting epoch:  371\n",
            "Avg Loss: 0.08035387843847275\n",
            "Starting epoch:  372\n",
            "Avg Loss: 0.08089036494493484\n",
            "Starting epoch:  373\n",
            "Avg Loss: 0.08019354194402695\n",
            "Starting epoch:  374\n",
            "Avg Loss: 0.07925233244895935\n",
            "Starting epoch:  375\n",
            "Avg Loss: 0.07961998134851456\n",
            "Starting epoch:  376\n",
            "Avg Loss: 0.07913951575756073\n",
            "Starting epoch:  377\n",
            "Avg Loss: 0.07887627184391022\n",
            "Starting epoch:  378\n",
            "Avg Loss: 0.07847662270069122\n",
            "Starting epoch:  379\n",
            "Avg Loss: 0.07949643582105637\n",
            "Starting epoch:  380\n",
            "Avg Loss: 0.07887446135282516\n",
            "Score: 20.139720558882235\n",
            "Starting epoch:  381\n",
            "Avg Loss: 0.07762553542852402\n",
            "Starting epoch:  382\n",
            "Avg Loss: 0.07842497527599335\n",
            "Starting epoch:  383\n",
            "Avg Loss: 0.07868877798318863\n",
            "Starting epoch:  384\n",
            "Avg Loss: 0.07867586612701416\n",
            "Starting epoch:  385\n",
            "Avg Loss: 0.07735869288444519\n",
            "Starting epoch:  386\n",
            "Avg Loss: 0.07687997072935104\n",
            "Starting epoch:  387\n",
            "Avg Loss: 0.07805922627449036\n",
            "Starting epoch:  388\n",
            "Avg Loss: 0.07781759649515152\n",
            "Starting epoch:  389\n",
            "Avg Loss: 0.07664517313241959\n",
            "Starting epoch:  390\n",
            "Avg Loss: 0.0771528109908104\n",
            "Score: 20.22811519817508\n",
            "Starting epoch:  391\n",
            "Avg Loss: 0.07720647752285004\n",
            "Starting epoch:  392\n",
            "Avg Loss: 0.07748708873987198\n",
            "Starting epoch:  393\n",
            "Avg Loss: 0.07701236754655838\n",
            "Starting epoch:  394\n",
            "Avg Loss: 0.0782799944281578\n",
            "Starting epoch:  395\n",
            "Avg Loss: 0.07707284390926361\n",
            "Starting epoch:  396\n",
            "Avg Loss: 0.07742199301719666\n",
            "Starting epoch:  397\n",
            "Avg Loss: 0.07653536647558212\n",
            "Starting epoch:  398\n",
            "Avg Loss: 0.0760912075638771\n",
            "Starting epoch:  399\n",
            "Avg Loss: 0.07634136825799942\n",
            "Starting epoch:  400\n",
            "Avg Loss: 0.07561048120260239\n",
            "Score: 20.293698317650414\n",
            "Starting epoch:  401\n",
            "Avg Loss: 0.07460720837116241\n",
            "Starting epoch:  402\n",
            "Avg Loss: 0.07543434947729111\n",
            "Starting epoch:  403\n",
            "Avg Loss: 0.07497362047433853\n",
            "Starting epoch:  404\n",
            "Avg Loss: 0.07543298602104187\n",
            "Starting epoch:  405\n",
            "Avg Loss: 0.07496875524520874\n",
            "Starting epoch:  406\n",
            "Avg Loss: 0.07437609881162643\n",
            "Starting epoch:  407\n",
            "Avg Loss: 0.07448669523000717\n",
            "Starting epoch:  408\n",
            "Avg Loss: 0.07555244117975235\n",
            "Starting epoch:  409\n",
            "Avg Loss: 0.0748882070183754\n",
            "Starting epoch:  410\n",
            "Avg Loss: 0.07440915703773499\n",
            "Score: 20.364984317080125\n",
            "Starting epoch:  411\n",
            "Avg Loss: 0.0744767040014267\n",
            "Starting epoch:  412\n",
            "Avg Loss: 0.07378645241260529\n",
            "Starting epoch:  413\n",
            "Avg Loss: 0.07398486137390137\n",
            "Starting epoch:  414\n",
            "Avg Loss: 0.07499735802412033\n",
            "Starting epoch:  415\n",
            "Avg Loss: 0.07436900585889816\n",
            "Starting epoch:  416\n",
            "Avg Loss: 0.07371404021978378\n",
            "Starting epoch:  417\n",
            "Avg Loss: 0.07291332632303238\n",
            "Starting epoch:  418\n",
            "Avg Loss: 0.07270915061235428\n",
            "Starting epoch:  419\n",
            "Avg Loss: 0.07319893687963486\n",
            "Starting epoch:  420\n",
            "Avg Loss: 0.07347168028354645\n",
            "Score: 20.493299116053606\n",
            "Starting epoch:  421\n",
            "Avg Loss: 0.0730530172586441\n",
            "Starting epoch:  422\n",
            "Avg Loss: 0.07303861528635025\n",
            "Starting epoch:  423\n",
            "Avg Loss: 0.07396606355905533\n",
            "Starting epoch:  424\n",
            "Avg Loss: 0.07265602052211761\n",
            "Starting epoch:  425\n",
            "Avg Loss: 0.07297870516777039\n",
            "Starting epoch:  426\n",
            "Avg Loss: 0.07259692251682281\n",
            "Starting epoch:  427\n",
            "Avg Loss: 0.07287608832120895\n",
            "Starting epoch:  428\n",
            "Avg Loss: 0.07233607023954391\n",
            "Starting epoch:  429\n",
            "Avg Loss: 0.07156652957201004\n",
            "Starting epoch:  430\n",
            "Avg Loss: 0.07221974432468414\n",
            "Score: 20.60450527516396\n",
            "Starting epoch:  431\n",
            "Avg Loss: 0.07109357416629791\n",
            "Starting epoch:  432\n",
            "Avg Loss: 0.07116976380348206\n",
            "Starting epoch:  433\n",
            "Avg Loss: 0.07052489370107651\n",
            "Starting epoch:  434\n",
            "Avg Loss: 0.07158844918012619\n",
            "Starting epoch:  435\n",
            "Avg Loss: 0.07265768945217133\n",
            "Starting epoch:  436\n",
            "Avg Loss: 0.07063362002372742\n",
            "Starting epoch:  437\n",
            "Avg Loss: 0.07103363424539566\n",
            "Starting epoch:  438\n",
            "Avg Loss: 0.07066292315721512\n",
            "Starting epoch:  439\n",
            "Avg Loss: 0.07092540711164474\n",
            "Starting epoch:  440\n",
            "Avg Loss: 0.06982944160699844\n",
            "Score: 20.681494154548048\n",
            "Starting epoch:  441\n",
            "Avg Loss: 0.07144905626773834\n",
            "Starting epoch:  442\n",
            "Avg Loss: 0.07028604298830032\n",
            "Starting epoch:  443\n",
            "Avg Loss: 0.0697426125407219\n",
            "Starting epoch:  444\n",
            "Avg Loss: 0.07039971649646759\n",
            "Starting epoch:  445\n",
            "Avg Loss: 0.0700048953294754\n",
            "Starting epoch:  446\n",
            "Avg Loss: 0.06979053467512131\n",
            "Starting epoch:  447\n",
            "Avg Loss: 0.06989285349845886\n",
            "Starting epoch:  448\n",
            "Avg Loss: 0.0704505443572998\n",
            "Starting epoch:  449\n",
            "Avg Loss: 0.06925085186958313\n",
            "Starting epoch:  450\n",
            "Avg Loss: 0.06985218077898026\n",
            "Score: 20.75563159395495\n",
            "Starting epoch:  451\n",
            "Avg Loss: 0.06941107660531998\n",
            "Starting epoch:  452\n",
            "Avg Loss: 0.0691923201084137\n",
            "Starting epoch:  453\n",
            "Avg Loss: 0.06910146027803421\n",
            "Starting epoch:  454\n",
            "Avg Loss: 0.06984391808509827\n",
            "Starting epoch:  455\n",
            "Avg Loss: 0.07001540809869766\n",
            "Starting epoch:  456\n",
            "Avg Loss: 0.06906065344810486\n",
            "Starting epoch:  457\n",
            "Avg Loss: 0.0681656152009964\n",
            "Starting epoch:  458\n",
            "Avg Loss: 0.06836551427841187\n",
            "Starting epoch:  459\n",
            "Avg Loss: 0.06856448203325272\n",
            "Starting epoch:  460\n",
            "Avg Loss: 0.06819671392440796\n",
            "Score: 20.80410607356715\n",
            "Starting epoch:  461\n",
            "Avg Loss: 0.06810630857944489\n",
            "Starting epoch:  462\n",
            "Avg Loss: 0.06856779009103775\n",
            "Starting epoch:  463\n",
            "Avg Loss: 0.06859292089939117\n",
            "Starting epoch:  464\n",
            "Avg Loss: 0.06893079727888107\n",
            "Starting epoch:  465\n",
            "Avg Loss: 0.06831300258636475\n",
            "Starting epoch:  466\n",
            "Avg Loss: 0.06773857027292252\n",
            "Starting epoch:  467\n",
            "Avg Loss: 0.06879804283380508\n",
            "Starting epoch:  468\n",
            "Avg Loss: 0.06749208271503448\n",
            "Starting epoch:  469\n",
            "Avg Loss: 0.06766045838594437\n",
            "Starting epoch:  470\n",
            "Avg Loss: 0.06818953901529312\n",
            "Score: 20.821214713430283\n",
            "Starting epoch:  471\n",
            "Avg Loss: 0.06682342290878296\n",
            "Starting epoch:  472\n",
            "Avg Loss: 0.06804969906806946\n",
            "Starting epoch:  473\n",
            "Avg Loss: 0.06679771840572357\n",
            "Starting epoch:  474\n",
            "Avg Loss: 0.06689078360795975\n",
            "Starting epoch:  475\n",
            "Avg Loss: 0.06746134161949158\n",
            "Starting epoch:  476\n",
            "Avg Loss: 0.06737793982028961\n",
            "Starting epoch:  477\n",
            "Avg Loss: 0.06655227392911911\n",
            "Starting epoch:  478\n",
            "Avg Loss: 0.06664672493934631\n",
            "Starting epoch:  479\n",
            "Avg Loss: 0.06645604968070984\n",
            "Starting epoch:  480\n",
            "Avg Loss: 0.06720653921365738\n",
            "Score: 20.88394639292843\n",
            "Starting epoch:  481\n",
            "Avg Loss: 0.06623028218746185\n",
            "Starting epoch:  482\n",
            "Avg Loss: 0.06678149104118347\n",
            "Starting epoch:  483\n",
            "Avg Loss: 0.06615512818098068\n",
            "Starting epoch:  484\n",
            "Avg Loss: 0.06518176943063736\n",
            "Starting epoch:  485\n",
            "Avg Loss: 0.06560582667589188\n",
            "Starting epoch:  486\n",
            "Avg Loss: 0.06623245030641556\n",
            "Starting epoch:  487\n",
            "Avg Loss: 0.06640920788049698\n",
            "Starting epoch:  488\n",
            "Avg Loss: 0.06557954847812653\n",
            "Starting epoch:  489\n",
            "Avg Loss: 0.06628291308879852\n",
            "Starting epoch:  490\n",
            "Avg Loss: 0.06686372309923172\n",
            "Score: 20.906757912745938\n",
            "Starting epoch:  491\n",
            "Avg Loss: 0.06583703309297562\n",
            "Starting epoch:  492\n",
            "Avg Loss: 0.06599026918411255\n",
            "Starting epoch:  493\n",
            "Avg Loss: 0.06587956100702286\n",
            "Starting epoch:  494\n",
            "Avg Loss: 0.06437859684228897\n",
            "Starting epoch:  495\n",
            "Avg Loss: 0.06583962589502335\n",
            "Starting epoch:  496\n",
            "Avg Loss: 0.06509211659431458\n",
            "Starting epoch:  497\n",
            "Avg Loss: 0.0653543546795845\n",
            "Starting epoch:  498\n",
            "Avg Loss: 0.06506435573101044\n",
            "Starting epoch:  499\n",
            "Avg Loss: 0.06502893567085266\n",
            "Starting epoch:  500\n",
            "Avg Loss: 0.06558386236429214\n",
            "Score: 21.00940975192472\n",
            "Starting epoch:  501\n",
            "Avg Loss: 0.06461367011070251\n",
            "Starting epoch:  502\n",
            "Avg Loss: 0.06515304744243622\n",
            "Starting epoch:  503\n",
            "Avg Loss: 0.06396304070949554\n",
            "Starting epoch:  504\n",
            "Avg Loss: 0.06486004590988159\n",
            "Starting epoch:  505\n",
            "Avg Loss: 0.06497164815664291\n",
            "Starting epoch:  506\n",
            "Avg Loss: 0.06388428807258606\n",
            "Starting epoch:  507\n",
            "Avg Loss: 0.0639704093337059\n",
            "Starting epoch:  508\n",
            "Avg Loss: 0.06390917301177979\n",
            "Starting epoch:  509\n",
            "Avg Loss: 0.06428300589323044\n",
            "Starting epoch:  510\n",
            "Avg Loss: 0.06403160840272903\n",
            "Score: 21.03222127174223\n",
            "Starting epoch:  511\n",
            "Avg Loss: 0.06331053376197815\n",
            "Starting epoch:  512\n",
            "Avg Loss: 0.06365222483873367\n",
            "Starting epoch:  513\n",
            "Avg Loss: 0.0640939474105835\n",
            "Starting epoch:  514\n",
            "Avg Loss: 0.06333113461732864\n",
            "Starting epoch:  515\n",
            "Avg Loss: 0.06435653567314148\n",
            "Starting epoch:  516\n",
            "Avg Loss: 0.06360336393117905\n",
            "Starting epoch:  517\n",
            "Avg Loss: 0.06370241940021515\n",
            "Starting epoch:  518\n",
            "Avg Loss: 0.062414541840553284\n",
            "Starting epoch:  519\n",
            "Avg Loss: 0.06327555328607559\n",
            "Starting epoch:  520\n",
            "Avg Loss: 0.06279829889535904\n",
            "Score: 21.109210151126316\n",
            "Starting epoch:  521\n",
            "Avg Loss: 0.06364868581295013\n",
            "Starting epoch:  522\n",
            "Avg Loss: 0.06341482698917389\n",
            "Starting epoch:  523\n",
            "Avg Loss: 0.06277433037757874\n",
            "Starting epoch:  524\n",
            "Avg Loss: 0.06334201991558075\n",
            "Starting epoch:  525\n",
            "Avg Loss: 0.06306588649749756\n",
            "Starting epoch:  526\n",
            "Avg Loss: 0.06279783695936203\n",
            "Starting epoch:  527\n",
            "Avg Loss: 0.06355046480894089\n",
            "Starting epoch:  528\n",
            "Avg Loss: 0.06282691657543182\n",
            "Starting epoch:  529\n",
            "Avg Loss: 0.06262744963169098\n",
            "Starting epoch:  530\n",
            "Avg Loss: 0.06278444081544876\n",
            "Score: 21.132021670943825\n",
            "Starting epoch:  531\n",
            "Avg Loss: 0.06205756589770317\n",
            "Starting epoch:  532\n",
            "Avg Loss: 0.06295564025640488\n",
            "Starting epoch:  533\n",
            "Avg Loss: 0.06274890154600143\n",
            "Starting epoch:  534\n",
            "Avg Loss: 0.06212471053004265\n",
            "Starting epoch:  535\n",
            "Avg Loss: 0.062189485877752304\n",
            "Starting epoch:  536\n",
            "Avg Loss: 0.06139418110251427\n",
            "Starting epoch:  537\n",
            "Avg Loss: 0.06118263304233551\n",
            "Starting epoch:  538\n",
            "Avg Loss: 0.061592962592840195\n",
            "Starting epoch:  539\n",
            "Avg Loss: 0.06150326877832413\n",
            "Starting epoch:  540\n",
            "Avg Loss: 0.061439864337444305\n",
            "Score: 21.19475335044197\n",
            "Starting epoch:  541\n",
            "Avg Loss: 0.061511896550655365\n",
            "Starting epoch:  542\n",
            "Avg Loss: 0.06203136965632439\n",
            "Starting epoch:  543\n",
            "Avg Loss: 0.06120378524065018\n",
            "Starting epoch:  544\n",
            "Avg Loss: 0.06182090565562248\n",
            "Starting epoch:  545\n",
            "Avg Loss: 0.06104135885834694\n",
            "Starting epoch:  546\n",
            "Avg Loss: 0.0610223226249218\n",
            "Starting epoch:  547\n",
            "Avg Loss: 0.06136896088719368\n",
            "Starting epoch:  548\n",
            "Avg Loss: 0.060896795243024826\n",
            "Starting epoch:  549\n",
            "Avg Loss: 0.06164790317416191\n",
            "Starting epoch:  550\n",
            "Avg Loss: 0.060810673981904984\n",
            "Score: 21.31451382948389\n",
            "Starting epoch:  551\n",
            "Avg Loss: 0.061748795211315155\n",
            "Starting epoch:  552\n",
            "Avg Loss: 0.06162825971841812\n",
            "Starting epoch:  553\n",
            "Avg Loss: 0.06095891818404198\n",
            "Starting epoch:  554\n",
            "Avg Loss: 0.06081293523311615\n",
            "Starting epoch:  555\n",
            "Avg Loss: 0.06083526462316513\n",
            "Starting epoch:  556\n",
            "Avg Loss: 0.060880694538354874\n",
            "Starting epoch:  557\n",
            "Avg Loss: 0.06105410307645798\n",
            "Starting epoch:  558\n",
            "Avg Loss: 0.060269467532634735\n",
            "Starting epoch:  559\n",
            "Avg Loss: 0.06070346012711525\n",
            "Starting epoch:  560\n",
            "Avg Loss: 0.05935490503907204\n",
            "Score: 21.297405189620758\n",
            "Starting epoch:  561\n",
            "Avg Loss: 0.06120031327009201\n",
            "Starting epoch:  562\n",
            "Avg Loss: 0.060012225061655045\n",
            "Starting epoch:  563\n",
            "Avg Loss: 0.05992570146918297\n",
            "Starting epoch:  564\n",
            "Avg Loss: 0.05982203781604767\n",
            "Starting epoch:  565\n",
            "Avg Loss: 0.06012982130050659\n",
            "Starting epoch:  566\n",
            "Avg Loss: 0.059978414326906204\n",
            "Starting epoch:  567\n",
            "Avg Loss: 0.06008906289935112\n",
            "Starting epoch:  568\n",
            "Avg Loss: 0.05999790132045746\n",
            "Starting epoch:  569\n",
            "Avg Loss: 0.0607704259455204\n",
            "Starting epoch:  570\n",
            "Avg Loss: 0.059431761503219604\n",
            "Score: 21.340176789278587\n",
            "Starting epoch:  571\n",
            "Avg Loss: 0.0594291053712368\n",
            "Starting epoch:  572\n",
            "Avg Loss: 0.05924484133720398\n",
            "Starting epoch:  573\n",
            "Avg Loss: 0.061057981103658676\n",
            "Starting epoch:  574\n",
            "Avg Loss: 0.059081751853227615\n",
            "Starting epoch:  575\n",
            "Avg Loss: 0.059012625366449356\n",
            "Starting epoch:  576\n",
            "Avg Loss: 0.058474645018577576\n",
            "Starting epoch:  577\n",
            "Avg Loss: 0.05957034230232239\n",
            "Starting epoch:  578\n",
            "Avg Loss: 0.0589667446911335\n",
            "Starting epoch:  579\n",
            "Avg Loss: 0.05943457782268524\n",
            "Starting epoch:  580\n",
            "Avg Loss: 0.058378610759973526\n",
            "Score: 21.40575990875392\n",
            "Starting epoch:  581\n",
            "Avg Loss: 0.05900919809937477\n",
            "Starting epoch:  582\n",
            "Avg Loss: 0.058158617466688156\n",
            "Starting epoch:  583\n",
            "Avg Loss: 0.05869615077972412\n",
            "Starting epoch:  584\n",
            "Avg Loss: 0.05823606252670288\n",
            "Starting epoch:  585\n",
            "Avg Loss: 0.05972388759255409\n",
            "Starting epoch:  586\n",
            "Avg Loss: 0.05779583379626274\n",
            "Starting epoch:  587\n",
            "Avg Loss: 0.058035753667354584\n",
            "Starting epoch:  588\n",
            "Avg Loss: 0.05873642861843109\n",
            "Starting epoch:  589\n",
            "Avg Loss: 0.0592602901160717\n",
            "Starting epoch:  590\n",
            "Avg Loss: 0.059098608791828156\n",
            "Score: 21.448531508411747\n",
            "Starting epoch:  591\n",
            "Avg Loss: 0.057626109570264816\n",
            "Starting epoch:  592\n",
            "Avg Loss: 0.05775914713740349\n",
            "Starting epoch:  593\n",
            "Avg Loss: 0.057596467435359955\n",
            "Starting epoch:  594\n",
            "Avg Loss: 0.058309562504291534\n",
            "Starting epoch:  595\n",
            "Avg Loss: 0.05838112160563469\n",
            "Starting epoch:  596\n",
            "Avg Loss: 0.058054119348526\n",
            "Starting epoch:  597\n",
            "Avg Loss: 0.05793720483779907\n",
            "Starting epoch:  598\n",
            "Avg Loss: 0.05779603496193886\n",
            "Starting epoch:  599\n",
            "Avg Loss: 0.05792636796832085\n",
            "Starting epoch:  600\n",
            "Avg Loss: 0.05730458348989487\n",
            "Score: 21.474194468206445\n",
            "Starting epoch:  601\n",
            "Avg Loss: 0.05797826871275902\n",
            "Starting epoch:  602\n",
            "Avg Loss: 0.058480679988861084\n",
            "Starting epoch:  603\n",
            "Avg Loss: 0.057756878435611725\n",
            "Starting epoch:  604\n",
            "Avg Loss: 0.05736164748668671\n",
            "Starting epoch:  605\n",
            "Avg Loss: 0.05750197172164917\n",
            "Starting epoch:  606\n",
            "Avg Loss: 0.05754372850060463\n",
            "Starting epoch:  607\n",
            "Avg Loss: 0.05826447904109955\n",
            "Starting epoch:  608\n",
            "Avg Loss: 0.05705719441175461\n",
            "Starting epoch:  609\n",
            "Avg Loss: 0.05760236084461212\n",
            "Starting epoch:  610\n",
            "Avg Loss: 0.05729961767792702\n",
            "Score: 21.477045908183634\n",
            "Starting epoch:  611\n",
            "Avg Loss: 0.057036224752664566\n",
            "Starting epoch:  612\n",
            "Avg Loss: 0.05802534893155098\n",
            "Starting epoch:  613\n",
            "Avg Loss: 0.056469619274139404\n",
            "Starting epoch:  614\n",
            "Avg Loss: 0.056914299726486206\n",
            "Starting epoch:  615\n",
            "Avg Loss: 0.056350644677877426\n",
            "Starting epoch:  616\n",
            "Avg Loss: 0.05745362117886543\n",
            "Starting epoch:  617\n",
            "Avg Loss: 0.05746787413954735\n",
            "Starting epoch:  618\n",
            "Avg Loss: 0.05763870105147362\n",
            "Starting epoch:  619\n",
            "Avg Loss: 0.05771622806787491\n",
            "Starting epoch:  620\n",
            "Avg Loss: 0.05714983865618706\n",
            "Score: 21.531223267750214\n",
            "Starting epoch:  621\n",
            "Avg Loss: 0.05729665979743004\n",
            "Starting epoch:  622\n",
            "Avg Loss: 0.05693574994802475\n",
            "Starting epoch:  623\n",
            "Avg Loss: 0.05614766478538513\n",
            "Starting epoch:  624\n",
            "Avg Loss: 0.05594919994473457\n",
            "Starting epoch:  625\n",
            "Avg Loss: 0.05659228935837746\n",
            "Starting epoch:  626\n",
            "Avg Loss: 0.05731693282723427\n",
            "Starting epoch:  627\n",
            "Avg Loss: 0.057081934064626694\n",
            "Starting epoch:  628\n",
            "Avg Loss: 0.05689755082130432\n",
            "Starting epoch:  629\n",
            "Avg Loss: 0.05603910610079765\n",
            "Starting epoch:  630\n",
            "Avg Loss: 0.056851815432310104\n",
            "Score: 21.53692614770459\n",
            "Starting epoch:  631\n",
            "Avg Loss: 0.05655059963464737\n",
            "Starting epoch:  632\n",
            "Avg Loss: 0.056607432663440704\n",
            "Starting epoch:  633\n",
            "Avg Loss: 0.05608212947845459\n",
            "Starting epoch:  634\n",
            "Avg Loss: 0.05580670014023781\n",
            "Starting epoch:  635\n",
            "Avg Loss: 0.05553586035966873\n",
            "Starting epoch:  636\n",
            "Avg Loss: 0.05609888583421707\n",
            "Starting epoch:  637\n",
            "Avg Loss: 0.055765677243471146\n",
            "Starting epoch:  638\n",
            "Avg Loss: 0.055156879127025604\n",
            "Starting epoch:  639\n",
            "Avg Loss: 0.05573384091258049\n",
            "Starting epoch:  640\n",
            "Avg Loss: 0.05545991659164429\n",
            "Score: 21.679498146564015\n",
            "Starting epoch:  641\n",
            "Avg Loss: 0.05653091147542\n",
            "Starting epoch:  642\n",
            "Avg Loss: 0.05601022019982338\n",
            "Starting epoch:  643\n",
            "Avg Loss: 0.056060049682855606\n",
            "Starting epoch:  644\n",
            "Avg Loss: 0.05594247579574585\n",
            "Starting epoch:  645\n",
            "Avg Loss: 0.05541277676820755\n",
            "Starting epoch:  646\n",
            "Avg Loss: 0.05549859255552292\n",
            "Starting epoch:  647\n",
            "Avg Loss: 0.05554228648543358\n",
            "Starting epoch:  648\n",
            "Avg Loss: 0.05653718113899231\n",
            "Starting epoch:  649\n",
            "Avg Loss: 0.05496987700462341\n",
            "Starting epoch:  650\n",
            "Avg Loss: 0.05486142262816429\n",
            "Score: 21.673795266609638\n",
            "Starting epoch:  651\n",
            "Avg Loss: 0.055263519287109375\n",
            "Starting epoch:  652\n",
            "Avg Loss: 0.05468311905860901\n",
            "Starting epoch:  653\n",
            "Avg Loss: 0.054810818284749985\n",
            "Starting epoch:  654\n",
            "Avg Loss: 0.055451348423957825\n",
            "Starting epoch:  655\n",
            "Avg Loss: 0.05577249825000763\n",
            "Starting epoch:  656\n",
            "Avg Loss: 0.05517057329416275\n",
            "Starting epoch:  657\n",
            "Avg Loss: 0.05528510734438896\n",
            "Starting epoch:  658\n",
            "Avg Loss: 0.055477481335401535\n",
            "Starting epoch:  659\n",
            "Avg Loss: 0.05479463189840317\n",
            "Starting epoch:  660\n",
            "Avg Loss: 0.05432915687561035\n",
            "Score: 21.747932706016538\n",
            "Starting epoch:  661\n",
            "Avg Loss: 0.0553508959710598\n",
            "Starting epoch:  662\n",
            "Avg Loss: 0.05459597334265709\n",
            "Starting epoch:  663\n",
            "Avg Loss: 0.05396124720573425\n",
            "Starting epoch:  664\n",
            "Avg Loss: 0.05495233088731766\n",
            "Starting epoch:  665\n",
            "Avg Loss: 0.05446505546569824\n",
            "Starting epoch:  666\n",
            "Avg Loss: 0.054523810744285583\n",
            "Starting epoch:  667\n",
            "Avg Loss: 0.05393141880631447\n",
            "Starting epoch:  668\n",
            "Avg Loss: 0.05394645407795906\n",
            "Starting epoch:  669\n",
            "Avg Loss: 0.05487945303320885\n",
            "Starting epoch:  670\n",
            "Avg Loss: 0.05400916934013367\n",
            "Score: 21.76218990590248\n",
            "Starting epoch:  671\n",
            "Avg Loss: 0.05485570430755615\n",
            "Starting epoch:  672\n",
            "Avg Loss: 0.054640110582113266\n",
            "Starting epoch:  673\n",
            "Avg Loss: 0.05421414226293564\n",
            "Starting epoch:  674\n",
            "Avg Loss: 0.054330166429281235\n",
            "Starting epoch:  675\n",
            "Avg Loss: 0.053773313760757446\n",
            "Starting epoch:  676\n",
            "Avg Loss: 0.054178714752197266\n",
            "Starting epoch:  677\n",
            "Avg Loss: 0.05379754304885864\n",
            "Starting epoch:  678\n",
            "Avg Loss: 0.054305195808410645\n",
            "Starting epoch:  679\n",
            "Avg Loss: 0.054486799985170364\n",
            "Starting epoch:  680\n",
            "Avg Loss: 0.05418456345796585\n",
            "Score: 21.830624465355005\n",
            "Starting epoch:  681\n",
            "Avg Loss: 0.05447497218847275\n",
            "Starting epoch:  682\n",
            "Avg Loss: 0.05341095104813576\n",
            "Starting epoch:  683\n",
            "Avg Loss: 0.054472293704748154\n",
            "Starting epoch:  684\n",
            "Avg Loss: 0.05386892333626747\n",
            "Starting epoch:  685\n",
            "Avg Loss: 0.05398637428879738\n",
            "Starting epoch:  686\n",
            "Avg Loss: 0.0537162609398365\n",
            "Starting epoch:  687\n",
            "Avg Loss: 0.054612547159194946\n",
            "Starting epoch:  688\n",
            "Avg Loss: 0.053430791944265366\n",
            "Starting epoch:  689\n",
            "Avg Loss: 0.053619347512722015\n",
            "Starting epoch:  690\n",
            "Avg Loss: 0.05306084081530571\n",
            "Score: 21.827773025377816\n",
            "Starting epoch:  691\n",
            "Avg Loss: 0.053159184753894806\n",
            "Starting epoch:  692\n",
            "Avg Loss: 0.053263746201992035\n",
            "Starting epoch:  693\n",
            "Avg Loss: 0.054328951984643936\n",
            "Starting epoch:  694\n",
            "Avg Loss: 0.052547816187143326\n",
            "Starting epoch:  695\n",
            "Avg Loss: 0.05362372845411301\n",
            "Starting epoch:  696\n",
            "Avg Loss: 0.05275193229317665\n",
            "Starting epoch:  697\n",
            "Avg Loss: 0.052383072674274445\n",
            "Starting epoch:  698\n",
            "Avg Loss: 0.05275240167975426\n",
            "Starting epoch:  699\n",
            "Avg Loss: 0.05314664542675018\n",
            "Starting epoch:  700\n",
            "Avg Loss: 0.053895577788352966\n",
            "Score: 21.824921585400627\n",
            "Starting epoch:  701\n",
            "Avg Loss: 0.05251079052686691\n",
            "Starting epoch:  702\n",
            "Avg Loss: 0.05309544503688812\n",
            "Starting epoch:  703\n",
            "Avg Loss: 0.054060135036706924\n",
            "Starting epoch:  704\n",
            "Avg Loss: 0.052975017577409744\n",
            "Starting epoch:  705\n",
            "Avg Loss: 0.05264721438288689\n",
            "Starting epoch:  706\n",
            "Avg Loss: 0.05361606180667877\n",
            "Starting epoch:  707\n",
            "Avg Loss: 0.052783142775297165\n",
            "Starting epoch:  708\n",
            "Avg Loss: 0.05317721515893936\n",
            "Starting epoch:  709\n",
            "Avg Loss: 0.05298926308751106\n",
            "Starting epoch:  710\n",
            "Avg Loss: 0.05266231670975685\n",
            "Score: 21.864841745081264\n",
            "Starting epoch:  711\n",
            "Avg Loss: 0.051891181617975235\n",
            "Starting epoch:  712\n",
            "Avg Loss: 0.052241526544094086\n",
            "Starting epoch:  713\n",
            "Avg Loss: 0.052504245191812515\n",
            "Starting epoch:  714\n",
            "Avg Loss: 0.05245213955640793\n",
            "Starting epoch:  715\n",
            "Avg Loss: 0.05269157141447067\n",
            "Starting epoch:  716\n",
            "Avg Loss: 0.05199195444583893\n",
            "Starting epoch:  717\n",
            "Avg Loss: 0.05219195783138275\n",
            "Starting epoch:  718\n",
            "Avg Loss: 0.05284872651100159\n",
            "Starting epoch:  719\n",
            "Avg Loss: 0.05236031487584114\n",
            "Starting epoch:  720\n",
            "Avg Loss: 0.05291762575507164\n",
            "Score: 21.86199030510408\n",
            "Starting epoch:  721\n",
            "Avg Loss: 0.05202041193842888\n",
            "Starting epoch:  722\n",
            "Avg Loss: 0.05234343186020851\n",
            "Starting epoch:  723\n",
            "Avg Loss: 0.052938371896743774\n",
            "Starting epoch:  724\n",
            "Avg Loss: 0.052181586623191833\n",
            "Starting epoch:  725\n",
            "Avg Loss: 0.05300465226173401\n",
            "Starting epoch:  726\n",
            "Avg Loss: 0.05220681428909302\n",
            "Starting epoch:  727\n",
            "Avg Loss: 0.05228042975068092\n",
            "Starting epoch:  728\n",
            "Avg Loss: 0.05203334614634514\n",
            "Starting epoch:  729\n",
            "Avg Loss: 0.05173274874687195\n",
            "Starting epoch:  730\n",
            "Avg Loss: 0.05285324901342392\n",
            "Score: 21.899059024807528\n",
            "Starting epoch:  731\n",
            "Avg Loss: 0.05198143422603607\n",
            "Starting epoch:  732\n",
            "Avg Loss: 0.05161033570766449\n",
            "Starting epoch:  733\n",
            "Avg Loss: 0.0517631359398365\n",
            "Starting epoch:  734\n",
            "Avg Loss: 0.05207325518131256\n",
            "Starting epoch:  735\n",
            "Avg Loss: 0.05155925825238228\n",
            "Starting epoch:  736\n",
            "Avg Loss: 0.05139745771884918\n",
            "Starting epoch:  737\n",
            "Avg Loss: 0.05164429172873497\n",
            "Starting epoch:  738\n",
            "Avg Loss: 0.05224606767296791\n",
            "Starting epoch:  739\n",
            "Avg Loss: 0.0513969361782074\n",
            "Starting epoch:  740\n",
            "Avg Loss: 0.051666226238012314\n",
            "Score: 21.89620758483034\n",
            "Starting epoch:  741\n",
            "Avg Loss: 0.05101644620299339\n",
            "Starting epoch:  742\n",
            "Avg Loss: 0.05170649290084839\n",
            "Starting epoch:  743\n",
            "Avg Loss: 0.05097169801592827\n",
            "Starting epoch:  744\n",
            "Avg Loss: 0.05044691637158394\n",
            "Starting epoch:  745\n",
            "Avg Loss: 0.05103449150919914\n",
            "Starting epoch:  746\n",
            "Avg Loss: 0.05147330090403557\n",
            "Starting epoch:  747\n",
            "Avg Loss: 0.050728172063827515\n",
            "Starting epoch:  748\n",
            "Avg Loss: 0.05188890919089317\n",
            "Starting epoch:  749\n",
            "Avg Loss: 0.05130218714475632\n",
            "Starting epoch:  750\n",
            "Avg Loss: 0.05081574618816376\n",
            "Score: 21.958939264328485\n",
            "Starting epoch:  751\n",
            "Avg Loss: 0.05102343112230301\n",
            "Starting epoch:  752\n",
            "Avg Loss: 0.0516163669526577\n",
            "Starting epoch:  753\n",
            "Avg Loss: 0.051659949123859406\n",
            "Starting epoch:  754\n",
            "Avg Loss: 0.05105425789952278\n",
            "Starting epoch:  755\n",
            "Avg Loss: 0.05105933919548988\n",
            "Starting epoch:  756\n",
            "Avg Loss: 0.05072633922100067\n",
            "Starting epoch:  757\n",
            "Avg Loss: 0.05050364509224892\n",
            "Starting epoch:  758\n",
            "Avg Loss: 0.05146666616201401\n",
            "Starting epoch:  759\n",
            "Avg Loss: 0.0511632040143013\n",
            "Starting epoch:  760\n",
            "Avg Loss: 0.05139220505952835\n",
            "Score: 21.998859424009122\n",
            "Starting epoch:  761\n",
            "Avg Loss: 0.05056515708565712\n",
            "Starting epoch:  762\n",
            "Avg Loss: 0.050268687307834625\n",
            "Starting epoch:  763\n",
            "Avg Loss: 0.051164425909519196\n",
            "Starting epoch:  764\n",
            "Avg Loss: 0.05138394609093666\n",
            "Starting epoch:  765\n",
            "Avg Loss: 0.05050534009933472\n",
            "Starting epoch:  766\n",
            "Avg Loss: 0.05006033182144165\n",
            "Starting epoch:  767\n",
            "Avg Loss: 0.049580495804548264\n",
            "Starting epoch:  768\n",
            "Avg Loss: 0.05066252872347832\n",
            "Starting epoch:  769\n",
            "Avg Loss: 0.050899192690849304\n",
            "Starting epoch:  770\n",
            "Avg Loss: 0.049812521785497665\n",
            "Score: 21.981750784145994\n",
            "Starting epoch:  771\n",
            "Avg Loss: 0.05047163739800453\n",
            "Starting epoch:  772\n",
            "Avg Loss: 0.049859631806612015\n",
            "Starting epoch:  773\n",
            "Avg Loss: 0.0490979366004467\n",
            "Starting epoch:  774\n",
            "Avg Loss: 0.05028707906603813\n",
            "Starting epoch:  775\n",
            "Avg Loss: 0.050596579909324646\n",
            "Starting epoch:  776\n",
            "Avg Loss: 0.049874331802129745\n",
            "Starting epoch:  777\n",
            "Avg Loss: 0.05042596906423569\n",
            "Starting epoch:  778\n",
            "Avg Loss: 0.050394393503665924\n",
            "Starting epoch:  779\n",
            "Avg Loss: 0.04927414283156395\n",
            "Starting epoch:  780\n",
            "Avg Loss: 0.049638789147138596\n",
            "Score: 22.038779583689763\n",
            "Starting epoch:  781\n",
            "Avg Loss: 0.049591224640607834\n",
            "Starting epoch:  782\n",
            "Avg Loss: 0.04954906180500984\n",
            "Starting epoch:  783\n",
            "Avg Loss: 0.0500662624835968\n",
            "Starting epoch:  784\n",
            "Avg Loss: 0.049846332520246506\n",
            "Starting epoch:  785\n",
            "Avg Loss: 0.049650803208351135\n",
            "Starting epoch:  786\n",
            "Avg Loss: 0.050710488110780716\n",
            "Starting epoch:  787\n",
            "Avg Loss: 0.04904631897807121\n",
            "Starting epoch:  788\n",
            "Avg Loss: 0.049953918904066086\n",
            "Starting epoch:  789\n",
            "Avg Loss: 0.05020095035433769\n",
            "Starting epoch:  790\n",
            "Avg Loss: 0.04997565224766731\n",
            "Score: 22.055888223552895\n",
            "Starting epoch:  791\n",
            "Avg Loss: 0.04983851686120033\n",
            "Starting epoch:  792\n",
            "Avg Loss: 0.04997289553284645\n",
            "Starting epoch:  793\n",
            "Avg Loss: 0.049516208469867706\n",
            "Starting epoch:  794\n",
            "Avg Loss: 0.04879037290811539\n",
            "Starting epoch:  795\n",
            "Avg Loss: 0.04924120754003525\n",
            "Starting epoch:  796\n",
            "Avg Loss: 0.049794696271419525\n",
            "Starting epoch:  797\n",
            "Avg Loss: 0.04941324517130852\n",
            "Starting epoch:  798\n",
            "Avg Loss: 0.04986068233847618\n",
            "Starting epoch:  799\n",
            "Avg Loss: 0.04938387870788574\n",
            "Starting epoch:  800\n",
            "Avg Loss: 0.049124546349048615\n",
            "Score: 22.095808383233535\n",
            "Starting epoch:  801\n",
            "Avg Loss: 0.048778217285871506\n",
            "Starting epoch:  802\n",
            "Avg Loss: 0.04928252100944519\n",
            "Starting epoch:  803\n",
            "Avg Loss: 0.04895688593387604\n",
            "Starting epoch:  804\n",
            "Avg Loss: 0.04866812750697136\n",
            "Starting epoch:  805\n",
            "Avg Loss: 0.04949336126446724\n",
            "Starting epoch:  806\n",
            "Avg Loss: 0.04937448352575302\n",
            "Starting epoch:  807\n",
            "Avg Loss: 0.04921576380729675\n",
            "Starting epoch:  808\n",
            "Avg Loss: 0.048986442387104034\n",
            "Starting epoch:  809\n",
            "Avg Loss: 0.04950068145990372\n",
            "Starting epoch:  810\n",
            "Avg Loss: 0.04982590675354004\n",
            "Score: 22.127174222982607\n",
            "Starting epoch:  811\n",
            "Avg Loss: 0.04891663044691086\n",
            "Starting epoch:  812\n",
            "Avg Loss: 0.0491548627614975\n",
            "Starting epoch:  813\n",
            "Avg Loss: 0.04834646359086037\n",
            "Starting epoch:  814\n",
            "Avg Loss: 0.049188803881406784\n",
            "Starting epoch:  815\n",
            "Avg Loss: 0.0493411086499691\n",
            "Starting epoch:  816\n",
            "Avg Loss: 0.048575304448604584\n",
            "Starting epoch:  817\n",
            "Avg Loss: 0.04893570393323898\n",
            "Starting epoch:  818\n",
            "Avg Loss: 0.04912899434566498\n",
            "Starting epoch:  819\n",
            "Avg Loss: 0.04863966256380081\n",
            "Starting epoch:  820\n",
            "Avg Loss: 0.049726370722055435\n",
            "Score: 22.17564870259481\n",
            "Starting epoch:  821\n",
            "Avg Loss: 0.04876227304339409\n",
            "Starting epoch:  822\n",
            "Avg Loss: 0.04851705953478813\n",
            "Starting epoch:  823\n",
            "Avg Loss: 0.04878407344222069\n",
            "Starting epoch:  824\n",
            "Avg Loss: 0.04860004037618637\n",
            "Starting epoch:  825\n",
            "Avg Loss: 0.04878311604261398\n",
            "Starting epoch:  826\n",
            "Avg Loss: 0.04769875481724739\n",
            "Starting epoch:  827\n",
            "Avg Loss: 0.048072006553411484\n",
            "Starting epoch:  828\n",
            "Avg Loss: 0.04883633553981781\n",
            "Starting epoch:  829\n",
            "Avg Loss: 0.0475713349878788\n",
            "Starting epoch:  830\n",
            "Avg Loss: 0.04794972762465477\n",
            "Score: 22.204163102366696\n",
            "Starting epoch:  831\n",
            "Avg Loss: 0.04786580055952072\n",
            "Starting epoch:  832\n",
            "Avg Loss: 0.048650436103343964\n",
            "Starting epoch:  833\n",
            "Avg Loss: 0.048362407833337784\n",
            "Starting epoch:  834\n",
            "Avg Loss: 0.04808858782052994\n",
            "Starting epoch:  835\n",
            "Avg Loss: 0.0483008474111557\n",
            "Starting epoch:  836\n",
            "Avg Loss: 0.04812037572264671\n",
            "Starting epoch:  837\n",
            "Avg Loss: 0.04861227795481682\n",
            "Starting epoch:  838\n",
            "Avg Loss: 0.04792037978768349\n",
            "Starting epoch:  839\n",
            "Avg Loss: 0.04860518127679825\n",
            "Starting epoch:  840\n",
            "Avg Loss: 0.04868025705218315\n",
            "Score: 22.221271742229824\n",
            "Starting epoch:  841\n",
            "Avg Loss: 0.04719053953886032\n",
            "Starting epoch:  842\n",
            "Avg Loss: 0.04788288101553917\n",
            "Starting epoch:  843\n",
            "Avg Loss: 0.04899434745311737\n",
            "Starting epoch:  844\n",
            "Avg Loss: 0.04773963242769241\n",
            "Starting epoch:  845\n",
            "Avg Loss: 0.04800496622920036\n",
            "Starting epoch:  846\n",
            "Avg Loss: 0.0478861927986145\n",
            "Starting epoch:  847\n",
            "Avg Loss: 0.048286497592926025\n",
            "Starting epoch:  848\n",
            "Avg Loss: 0.04720667377114296\n",
            "Starting epoch:  849\n",
            "Avg Loss: 0.04749123007059097\n",
            "Starting epoch:  850\n",
            "Avg Loss: 0.047434400767087936\n",
            "Score: 22.255489021956087\n",
            "Starting epoch:  851\n",
            "Avg Loss: 0.04759829118847847\n",
            "Starting epoch:  852\n",
            "Avg Loss: 0.04817622900009155\n",
            "Starting epoch:  853\n",
            "Avg Loss: 0.04782543331384659\n",
            "Starting epoch:  854\n",
            "Avg Loss: 0.04765123501420021\n",
            "Starting epoch:  855\n",
            "Avg Loss: 0.04790298268198967\n",
            "Starting epoch:  856\n",
            "Avg Loss: 0.04788496717810631\n",
            "Starting epoch:  857\n",
            "Avg Loss: 0.04772338643670082\n",
            "Starting epoch:  858\n",
            "Avg Loss: 0.047888923436403275\n",
            "Starting epoch:  859\n",
            "Avg Loss: 0.04842957481741905\n",
            "Starting epoch:  860\n",
            "Avg Loss: 0.047809142619371414\n",
            "Score: 22.31251782149986\n",
            "Starting epoch:  861\n",
            "Avg Loss: 0.048022232949733734\n",
            "Starting epoch:  862\n",
            "Avg Loss: 0.04670354351401329\n",
            "Starting epoch:  863\n",
            "Avg Loss: 0.04769724979996681\n",
            "Starting epoch:  864\n",
            "Avg Loss: 0.046902306377887726\n",
            "Starting epoch:  865\n",
            "Avg Loss: 0.047247253358364105\n",
            "Starting epoch:  866\n",
            "Avg Loss: 0.04710780829191208\n",
            "Starting epoch:  867\n",
            "Avg Loss: 0.04709773138165474\n",
            "Starting epoch:  868\n",
            "Avg Loss: 0.0476870983839035\n",
            "Starting epoch:  869\n",
            "Avg Loss: 0.046904683113098145\n",
            "Starting epoch:  870\n",
            "Avg Loss: 0.0476846769452095\n",
            "Score: 22.321072141431422\n",
            "Starting epoch:  871\n",
            "Avg Loss: 0.04692360386252403\n",
            "Starting epoch:  872\n",
            "Avg Loss: 0.04738881066441536\n",
            "Starting epoch:  873\n",
            "Avg Loss: 0.047938644886016846\n",
            "Starting epoch:  874\n",
            "Avg Loss: 0.04708543419837952\n",
            "Starting epoch:  875\n",
            "Avg Loss: 0.0469372384250164\n",
            "Starting epoch:  876\n",
            "Avg Loss: 0.047243475914001465\n",
            "Starting epoch:  877\n",
            "Avg Loss: 0.04726181551814079\n",
            "Starting epoch:  878\n",
            "Avg Loss: 0.047552697360515594\n",
            "Starting epoch:  879\n",
            "Avg Loss: 0.04746002331376076\n",
            "Starting epoch:  880\n",
            "Avg Loss: 0.04740919545292854\n",
            "Score: 22.30396350156829\n",
            "Starting epoch:  881\n",
            "Avg Loss: 0.04716372489929199\n",
            "Starting epoch:  882\n",
            "Avg Loss: 0.04637257382273674\n",
            "Starting epoch:  883\n",
            "Avg Loss: 0.04736389219760895\n",
            "Starting epoch:  884\n",
            "Avg Loss: 0.04634074866771698\n",
            "Starting epoch:  885\n",
            "Avg Loss: 0.0471196286380291\n",
            "Starting epoch:  886\n",
            "Avg Loss: 0.047412171959877014\n",
            "Starting epoch:  887\n",
            "Avg Loss: 0.046221062541007996\n",
            "Starting epoch:  888\n",
            "Avg Loss: 0.047266144305467606\n",
            "Starting epoch:  889\n",
            "Avg Loss: 0.04639134556055069\n",
            "Starting epoch:  890\n",
            "Avg Loss: 0.046074625104665756\n",
            "Score: 22.369546621043625\n",
            "Starting epoch:  891\n",
            "Avg Loss: 0.04648764804005623\n",
            "Starting epoch:  892\n",
            "Avg Loss: 0.04650992527604103\n",
            "Starting epoch:  893\n",
            "Avg Loss: 0.04676712676882744\n",
            "Starting epoch:  894\n",
            "Avg Loss: 0.046639829874038696\n",
            "Starting epoch:  895\n",
            "Avg Loss: 0.046616312116384506\n",
            "Starting epoch:  896\n",
            "Avg Loss: 0.04608597233891487\n",
            "Starting epoch:  897\n",
            "Avg Loss: 0.047075238078832626\n",
            "Starting epoch:  898\n",
            "Avg Loss: 0.04730180278420448\n",
            "Starting epoch:  899\n",
            "Avg Loss: 0.046805523335933685\n",
            "Starting epoch:  900\n",
            "Avg Loss: 0.04652286693453789\n",
            "Score: 22.380952380952383\n",
            "Starting epoch:  901\n",
            "Avg Loss: 0.045443207025527954\n",
            "Starting epoch:  902\n",
            "Avg Loss: 0.046179383993148804\n",
            "Starting epoch:  903\n",
            "Avg Loss: 0.04685240611433983\n",
            "Starting epoch:  904\n",
            "Avg Loss: 0.045955806970596313\n",
            "Starting epoch:  905\n",
            "Avg Loss: 0.046717796474695206\n",
            "Starting epoch:  906\n",
            "Avg Loss: 0.04607811197638512\n",
            "Starting epoch:  907\n",
            "Avg Loss: 0.04690210893750191\n",
            "Starting epoch:  908\n",
            "Avg Loss: 0.04676283150911331\n",
            "Starting epoch:  909\n",
            "Avg Loss: 0.046409182250499725\n",
            "Starting epoch:  910\n",
            "Avg Loss: 0.046631015837192535\n",
            "Score: 22.383803820929568\n",
            "Starting epoch:  911\n",
            "Avg Loss: 0.04685812070965767\n",
            "Starting epoch:  912\n",
            "Avg Loss: 0.046578411012887955\n",
            "Starting epoch:  913\n",
            "Avg Loss: 0.04645466059446335\n",
            "Starting epoch:  914\n",
            "Avg Loss: 0.046327877789735794\n",
            "Starting epoch:  915\n",
            "Avg Loss: 0.046367842704057693\n",
            "Starting epoch:  916\n",
            "Avg Loss: 0.04573468491435051\n",
            "Starting epoch:  917\n",
            "Avg Loss: 0.045947879552841187\n",
            "Starting epoch:  918\n",
            "Avg Loss: 0.04523337259888649\n",
            "Starting epoch:  919\n",
            "Avg Loss: 0.046084944158792496\n",
            "Starting epoch:  920\n",
            "Avg Loss: 0.04605666548013687\n",
            "Score: 22.41802110065583\n",
            "Starting epoch:  921\n",
            "Avg Loss: 0.04534837603569031\n",
            "Starting epoch:  922\n",
            "Avg Loss: 0.04631514474749565\n",
            "Starting epoch:  923\n",
            "Avg Loss: 0.04614526405930519\n",
            "Starting epoch:  924\n",
            "Avg Loss: 0.046268802136182785\n",
            "Starting epoch:  925\n",
            "Avg Loss: 0.04590952768921852\n",
            "Starting epoch:  926\n",
            "Avg Loss: 0.04508810117840767\n",
            "Starting epoch:  927\n",
            "Avg Loss: 0.04555462300777435\n",
            "Starting epoch:  928\n",
            "Avg Loss: 0.045928552746772766\n",
            "Starting epoch:  929\n",
            "Avg Loss: 0.04595909267663956\n",
            "Starting epoch:  930\n",
            "Avg Loss: 0.045293934643268585\n",
            "Score: 22.463644140290846\n",
            "Starting epoch:  931\n",
            "Avg Loss: 0.04551813751459122\n",
            "Starting epoch:  932\n",
            "Avg Loss: 0.04632546007633209\n",
            "Starting epoch:  933\n",
            "Avg Loss: 0.04625147208571434\n",
            "Starting epoch:  934\n",
            "Avg Loss: 0.04523260518908501\n",
            "Starting epoch:  935\n",
            "Avg Loss: 0.045784227550029755\n",
            "Starting epoch:  936\n",
            "Avg Loss: 0.04603853076696396\n",
            "Starting epoch:  937\n",
            "Avg Loss: 0.04519857093691826\n",
            "Starting epoch:  938\n",
            "Avg Loss: 0.045957379043102264\n",
            "Starting epoch:  939\n",
            "Avg Loss: 0.04560722038149834\n",
            "Starting epoch:  940\n",
            "Avg Loss: 0.04587927088141441\n",
            "Score: 22.45794126033647\n",
            "Starting epoch:  941\n",
            "Avg Loss: 0.04529881477355957\n",
            "Starting epoch:  942\n",
            "Avg Loss: 0.04551924020051956\n",
            "Starting epoch:  943\n",
            "Avg Loss: 0.04547394812107086\n",
            "Starting epoch:  944\n",
            "Avg Loss: 0.045168932527303696\n",
            "Starting epoch:  945\n",
            "Avg Loss: 0.044752322137355804\n",
            "Starting epoch:  946\n",
            "Avg Loss: 0.044688571244478226\n",
            "Starting epoch:  947\n",
            "Avg Loss: 0.04590786620974541\n",
            "Starting epoch:  948\n",
            "Avg Loss: 0.04531567171216011\n",
            "Starting epoch:  949\n",
            "Avg Loss: 0.04508822038769722\n",
            "Starting epoch:  950\n",
            "Avg Loss: 0.04464198276400566\n",
            "Score: 22.503564299971483\n",
            "Starting epoch:  951\n",
            "Avg Loss: 0.045602332800626755\n",
            "Starting epoch:  952\n",
            "Avg Loss: 0.04448753967881203\n",
            "Starting epoch:  953\n",
            "Avg Loss: 0.04509955644607544\n",
            "Starting epoch:  954\n",
            "Avg Loss: 0.04530097916722298\n",
            "Starting epoch:  955\n",
            "Avg Loss: 0.04435937479138374\n",
            "Starting epoch:  956\n",
            "Avg Loss: 0.04530530795454979\n",
            "Starting epoch:  957\n",
            "Avg Loss: 0.045076098293066025\n",
            "Starting epoch:  958\n",
            "Avg Loss: 0.04446187615394592\n",
            "Starting epoch:  959\n",
            "Avg Loss: 0.04555881395936012\n",
            "Starting epoch:  960\n",
            "Avg Loss: 0.044746339321136475\n",
            "Score: 22.466495580268038\n",
            "Starting epoch:  961\n",
            "Avg Loss: 0.04567892849445343\n",
            "Starting epoch:  962\n",
            "Avg Loss: 0.04505229741334915\n",
            "Starting epoch:  963\n",
            "Avg Loss: 0.04487759247422218\n",
            "Starting epoch:  964\n",
            "Avg Loss: 0.044333502650260925\n",
            "Starting epoch:  965\n",
            "Avg Loss: 0.04556703194975853\n",
            "Starting epoch:  966\n",
            "Avg Loss: 0.04487774148583412\n",
            "Starting epoch:  967\n",
            "Avg Loss: 0.044372234493494034\n",
            "Starting epoch:  968\n",
            "Avg Loss: 0.04499443992972374\n",
            "Starting epoch:  969\n",
            "Avg Loss: 0.04420000687241554\n",
            "Starting epoch:  970\n",
            "Avg Loss: 0.04478401318192482\n",
            "Score: 22.540633019674935\n",
            "Starting epoch:  971\n",
            "Avg Loss: 0.044970933347940445\n",
            "Starting epoch:  972\n",
            "Avg Loss: 0.04493086040019989\n",
            "Starting epoch:  973\n",
            "Avg Loss: 0.044285304844379425\n",
            "Starting epoch:  974\n",
            "Avg Loss: 0.04458434134721756\n",
            "Starting epoch:  975\n",
            "Avg Loss: 0.04531008377671242\n",
            "Starting epoch:  976\n",
            "Avg Loss: 0.04448045417666435\n",
            "Starting epoch:  977\n",
            "Avg Loss: 0.04521133005619049\n",
            "Starting epoch:  978\n",
            "Avg Loss: 0.04435296356678009\n",
            "Starting epoch:  979\n",
            "Avg Loss: 0.04447636008262634\n",
            "Starting epoch:  980\n",
            "Avg Loss: 0.044671930372714996\n",
            "Score: 22.617621899059024\n",
            "Starting epoch:  981\n",
            "Avg Loss: 0.04464995115995407\n",
            "Starting epoch:  982\n",
            "Avg Loss: 0.04481635242700577\n",
            "Starting epoch:  983\n",
            "Avg Loss: 0.04391976818442345\n",
            "Starting epoch:  984\n",
            "Avg Loss: 0.043892376124858856\n",
            "Starting epoch:  985\n",
            "Avg Loss: 0.04450339451432228\n",
            "Starting epoch:  986\n",
            "Avg Loss: 0.04414337873458862\n",
            "Starting epoch:  987\n",
            "Avg Loss: 0.04391494765877724\n",
            "Starting epoch:  988\n",
            "Avg Loss: 0.0443870984017849\n",
            "Starting epoch:  989\n",
            "Avg Loss: 0.044565387070178986\n",
            "Starting epoch:  990\n",
            "Avg Loss: 0.04483531787991524\n",
            "Score: 22.620473339036213\n",
            "Starting epoch:  991\n",
            "Avg Loss: 0.04382980614900589\n",
            "Starting epoch:  992\n",
            "Avg Loss: 0.04493877664208412\n",
            "Starting epoch:  993\n",
            "Avg Loss: 0.04465384781360626\n",
            "Starting epoch:  994\n",
            "Avg Loss: 0.04431333392858505\n",
            "Starting epoch:  995\n",
            "Avg Loss: 0.044688545167446136\n",
            "Starting epoch:  996\n",
            "Avg Loss: 0.043259698897600174\n",
            "Starting epoch:  997\n",
            "Avg Loss: 0.04460025578737259\n",
            "Starting epoch:  998\n",
            "Avg Loss: 0.04396313056349754\n",
            "Starting epoch:  999\n",
            "Avg Loss: 0.043929897248744965\n",
            "Starting epoch:  1000\n",
            "Avg Loss: 0.043706901371479034\n",
            "Score: 22.597661819218708\n",
            "Starting epoch:  1001\n",
            "Avg Loss: 0.044266585260629654\n",
            "Starting epoch:  1002\n",
            "Avg Loss: 0.043973591178655624\n",
            "Starting epoch:  1003\n",
            "Avg Loss: 0.044338107109069824\n",
            "Starting epoch:  1004\n",
            "Avg Loss: 0.04464837536215782\n",
            "Starting epoch:  1005\n",
            "Avg Loss: 0.04328320920467377\n",
            "Starting epoch:  1006\n",
            "Avg Loss: 0.04415011405944824\n",
            "Starting epoch:  1007\n",
            "Avg Loss: 0.044077079743146896\n",
            "Starting epoch:  1008\n",
            "Avg Loss: 0.04397845268249512\n",
            "Starting epoch:  1009\n",
            "Avg Loss: 0.04342024400830269\n",
            "Starting epoch:  1010\n",
            "Avg Loss: 0.04422152414917946\n",
            "Score: 22.617621899059024\n",
            "Starting epoch:  1011\n",
            "Avg Loss: 0.04420231282711029\n",
            "Starting epoch:  1012\n",
            "Avg Loss: 0.04370009899139404\n",
            "Starting epoch:  1013\n",
            "Avg Loss: 0.04368782415986061\n",
            "Starting epoch:  1014\n",
            "Avg Loss: 0.04336954280734062\n",
            "Starting epoch:  1015\n",
            "Avg Loss: 0.04413466528058052\n",
            "Starting epoch:  1016\n",
            "Avg Loss: 0.04346249997615814\n",
            "Starting epoch:  1017\n",
            "Avg Loss: 0.04421692341566086\n",
            "Starting epoch:  1018\n",
            "Avg Loss: 0.04348873347043991\n",
            "Starting epoch:  1019\n",
            "Avg Loss: 0.04400769621133804\n",
            "Starting epoch:  1020\n",
            "Avg Loss: 0.043342895805835724\n",
            "Score: 22.631879098944967\n",
            "Starting epoch:  1021\n",
            "Avg Loss: 0.043471332639455795\n",
            "Starting epoch:  1022\n",
            "Avg Loss: 0.04368642717599869\n",
            "Starting epoch:  1023\n",
            "Avg Loss: 0.044216591864824295\n",
            "Starting epoch:  1024\n",
            "Avg Loss: 0.04297950491309166\n",
            "Starting epoch:  1025\n",
            "Avg Loss: 0.04378113895654678\n",
            "Starting epoch:  1026\n",
            "Avg Loss: 0.043836161494255066\n",
            "Starting epoch:  1027\n",
            "Avg Loss: 0.043997664004564285\n",
            "Starting epoch:  1028\n",
            "Avg Loss: 0.042983781546354294\n",
            "Starting epoch:  1029\n",
            "Avg Loss: 0.04390385001897812\n",
            "Starting epoch:  1030\n",
            "Avg Loss: 0.04400976002216339\n",
            "Score: 22.62617621899059\n",
            "Starting epoch:  1031\n",
            "Avg Loss: 0.0428079217672348\n",
            "Starting epoch:  1032\n",
            "Avg Loss: 0.042702481150627136\n",
            "Starting epoch:  1033\n",
            "Avg Loss: 0.04304968938231468\n",
            "Starting epoch:  1034\n",
            "Avg Loss: 0.04283253476023674\n",
            "Starting epoch:  1035\n",
            "Avg Loss: 0.043184008449316025\n",
            "Starting epoch:  1036\n",
            "Avg Loss: 0.0430428646504879\n",
            "Starting epoch:  1037\n",
            "Avg Loss: 0.04357662424445152\n",
            "Starting epoch:  1038\n",
            "Avg Loss: 0.042653027921915054\n",
            "Starting epoch:  1039\n",
            "Avg Loss: 0.04342946037650108\n",
            "Starting epoch:  1040\n",
            "Avg Loss: 0.04312911257147789\n",
            "Score: 22.660393498716854\n",
            "Starting epoch:  1041\n",
            "Avg Loss: 0.043664783239364624\n",
            "Starting epoch:  1042\n",
            "Avg Loss: 0.04358348995447159\n",
            "Starting epoch:  1043\n",
            "Avg Loss: 0.042336735874414444\n",
            "Starting epoch:  1044\n",
            "Avg Loss: 0.043651312589645386\n",
            "Starting epoch:  1045\n",
            "Avg Loss: 0.04340619593858719\n",
            "Starting epoch:  1046\n",
            "Avg Loss: 0.04279410466551781\n",
            "Starting epoch:  1047\n",
            "Avg Loss: 0.04346887022256851\n",
            "Starting epoch:  1048\n",
            "Avg Loss: 0.042959339916706085\n",
            "Starting epoch:  1049\n",
            "Avg Loss: 0.04357483610510826\n",
            "Starting epoch:  1050\n",
            "Avg Loss: 0.04299772158265114\n",
            "Score: 22.688907898488736\n",
            "Starting epoch:  1051\n",
            "Avg Loss: 0.043490830808877945\n",
            "Starting epoch:  1052\n",
            "Avg Loss: 0.04288002848625183\n",
            "Starting epoch:  1053\n",
            "Avg Loss: 0.04334498941898346\n",
            "Starting epoch:  1054\n",
            "Avg Loss: 0.0431930273771286\n",
            "Starting epoch:  1055\n",
            "Avg Loss: 0.043621063232421875\n",
            "Starting epoch:  1056\n",
            "Avg Loss: 0.04240800067782402\n",
            "Starting epoch:  1057\n",
            "Avg Loss: 0.04388407617807388\n",
            "Starting epoch:  1058\n",
            "Avg Loss: 0.042934972792863846\n",
            "Starting epoch:  1059\n",
            "Avg Loss: 0.04306384548544884\n",
            "Starting epoch:  1060\n",
            "Avg Loss: 0.04285910353064537\n",
            "Score: 22.72597661819219\n",
            "Starting epoch:  1061\n",
            "Avg Loss: 0.04363449662923813\n",
            "Starting epoch:  1062\n",
            "Avg Loss: 0.042822085320949554\n",
            "Starting epoch:  1063\n",
            "Avg Loss: 0.042831748723983765\n",
            "Starting epoch:  1064\n",
            "Avg Loss: 0.04228951781988144\n",
            "Starting epoch:  1065\n",
            "Avg Loss: 0.042942218482494354\n",
            "Starting epoch:  1066\n",
            "Avg Loss: 0.04304539039731026\n",
            "Starting epoch:  1067\n",
            "Avg Loss: 0.042757801711559296\n",
            "Starting epoch:  1068\n",
            "Avg Loss: 0.04264036938548088\n",
            "Starting epoch:  1069\n",
            "Avg Loss: 0.042742107063531876\n",
            "Starting epoch:  1070\n",
            "Avg Loss: 0.04280644655227661\n",
            "Score: 22.740233818078128\n",
            "Starting epoch:  1071\n",
            "Avg Loss: 0.04253923147916794\n",
            "Starting epoch:  1072\n",
            "Avg Loss: 0.04299984127283096\n",
            "Starting epoch:  1073\n",
            "Avg Loss: 0.04264473915100098\n",
            "Starting epoch:  1074\n",
            "Avg Loss: 0.0426134318113327\n",
            "Starting epoch:  1075\n",
            "Avg Loss: 0.04273095726966858\n",
            "Starting epoch:  1076\n",
            "Avg Loss: 0.04197096824645996\n",
            "Starting epoch:  1077\n",
            "Avg Loss: 0.04275382682681084\n",
            "Starting epoch:  1078\n",
            "Avg Loss: 0.04214407131075859\n",
            "Starting epoch:  1079\n",
            "Avg Loss: 0.0424998514354229\n",
            "Starting epoch:  1080\n",
            "Avg Loss: 0.04246225580573082\n",
            "Score: 22.788708297690334\n",
            "Starting epoch:  1081\n",
            "Avg Loss: 0.042499981820583344\n",
            "Starting epoch:  1082\n",
            "Avg Loss: 0.04357699677348137\n",
            "Starting epoch:  1083\n",
            "Avg Loss: 0.042797792702913284\n",
            "Starting epoch:  1084\n",
            "Avg Loss: 0.042334020137786865\n",
            "Starting epoch:  1085\n",
            "Avg Loss: 0.04241003096103668\n",
            "Starting epoch:  1086\n",
            "Avg Loss: 0.04320293292403221\n",
            "Starting epoch:  1087\n",
            "Avg Loss: 0.04235150292515755\n",
            "Starting epoch:  1088\n",
            "Avg Loss: 0.042731933295726776\n",
            "Starting epoch:  1089\n",
            "Avg Loss: 0.0426839217543602\n",
            "Starting epoch:  1090\n",
            "Avg Loss: 0.04215920716524124\n",
            "Score: 22.811519817507843\n",
            "Starting epoch:  1091\n",
            "Avg Loss: 0.042341601103544235\n",
            "Starting epoch:  1092\n",
            "Avg Loss: 0.04206785932183266\n",
            "Starting epoch:  1093\n",
            "Avg Loss: 0.042357783764600754\n",
            "Starting epoch:  1094\n",
            "Avg Loss: 0.04253368824720383\n",
            "Starting epoch:  1095\n",
            "Avg Loss: 0.0419011116027832\n",
            "Starting epoch:  1096\n",
            "Avg Loss: 0.04282915219664574\n",
            "Starting epoch:  1097\n",
            "Avg Loss: 0.04310183227062225\n",
            "Starting epoch:  1098\n",
            "Avg Loss: 0.04276514798402786\n",
            "Starting epoch:  1099\n",
            "Avg Loss: 0.042449191212654114\n",
            "Starting epoch:  1100\n",
            "Avg Loss: 0.04223813861608505\n",
            "Score: 22.814371257485032\n",
            "Starting epoch:  1101\n",
            "Avg Loss: 0.042562928050756454\n",
            "Starting epoch:  1102\n",
            "Avg Loss: 0.04187242314219475\n",
            "Starting epoch:  1103\n",
            "Avg Loss: 0.04261937737464905\n",
            "Starting epoch:  1104\n",
            "Avg Loss: 0.0422840341925621\n",
            "Starting epoch:  1105\n",
            "Avg Loss: 0.04273640364408493\n",
            "Starting epoch:  1106\n",
            "Avg Loss: 0.0425744466483593\n",
            "Starting epoch:  1107\n",
            "Avg Loss: 0.0419553704559803\n",
            "Starting epoch:  1108\n",
            "Avg Loss: 0.04247327893972397\n",
            "Starting epoch:  1109\n",
            "Avg Loss: 0.042094580829143524\n",
            "Starting epoch:  1110\n",
            "Avg Loss: 0.041995540261268616\n",
            "Score: 22.85429141716567\n",
            "Starting epoch:  1111\n",
            "Avg Loss: 0.0425129272043705\n",
            "Starting epoch:  1112\n",
            "Avg Loss: 0.04177175462245941\n",
            "Starting epoch:  1113\n",
            "Avg Loss: 0.042178913950920105\n",
            "Starting epoch:  1114\n",
            "Avg Loss: 0.04182795062661171\n",
            "Starting epoch:  1115\n",
            "Avg Loss: 0.04215935617685318\n",
            "Starting epoch:  1116\n",
            "Avg Loss: 0.0422351248562336\n",
            "Starting epoch:  1117\n",
            "Avg Loss: 0.04192538931965828\n",
            "Starting epoch:  1118\n",
            "Avg Loss: 0.041522711515426636\n",
            "Starting epoch:  1119\n",
            "Avg Loss: 0.04155217483639717\n",
            "Starting epoch:  1120\n",
            "Avg Loss: 0.04169733077287674\n",
            "Score: 22.891360136869118\n",
            "Starting epoch:  1121\n",
            "Avg Loss: 0.041481949388980865\n",
            "Starting epoch:  1122\n",
            "Avg Loss: 0.04144797846674919\n",
            "Starting epoch:  1123\n",
            "Avg Loss: 0.042102742940187454\n",
            "Starting epoch:  1124\n",
            "Avg Loss: 0.0424216091632843\n",
            "Starting epoch:  1125\n",
            "Avg Loss: 0.04244276136159897\n",
            "Starting epoch:  1126\n",
            "Avg Loss: 0.04236457869410515\n",
            "Starting epoch:  1127\n",
            "Avg Loss: 0.041268885135650635\n",
            "Starting epoch:  1128\n",
            "Avg Loss: 0.04174726456403732\n",
            "Starting epoch:  1129\n",
            "Avg Loss: 0.04176245257258415\n",
            "Starting epoch:  1130\n",
            "Avg Loss: 0.04111222177743912\n",
            "Score: 22.931280296549758\n",
            "Starting epoch:  1131\n",
            "Avg Loss: 0.04161154851317406\n",
            "Starting epoch:  1132\n",
            "Avg Loss: 0.04145221784710884\n",
            "Starting epoch:  1133\n",
            "Avg Loss: 0.041835930198431015\n",
            "Starting epoch:  1134\n",
            "Avg Loss: 0.041306763887405396\n",
            "Starting epoch:  1135\n",
            "Avg Loss: 0.04091767221689224\n",
            "Starting epoch:  1136\n",
            "Avg Loss: 0.04202308878302574\n",
            "Starting epoch:  1137\n",
            "Avg Loss: 0.04152410477399826\n",
            "Starting epoch:  1138\n",
            "Avg Loss: 0.04161759465932846\n",
            "Starting epoch:  1139\n",
            "Avg Loss: 0.040749531239271164\n",
            "Starting epoch:  1140\n",
            "Avg Loss: 0.04234948754310608\n",
            "Score: 22.98545765611634\n",
            "Starting epoch:  1141\n",
            "Avg Loss: 0.04179345443844795\n",
            "Starting epoch:  1142\n",
            "Avg Loss: 0.040818456560373306\n",
            "Starting epoch:  1143\n",
            "Avg Loss: 0.04148390144109726\n",
            "Starting epoch:  1144\n",
            "Avg Loss: 0.04149160534143448\n",
            "Starting epoch:  1145\n",
            "Avg Loss: 0.04115260764956474\n",
            "Starting epoch:  1146\n",
            "Avg Loss: 0.04186950996518135\n",
            "Starting epoch:  1147\n",
            "Avg Loss: 0.042417556047439575\n",
            "Starting epoch:  1148\n",
            "Avg Loss: 0.041468314826488495\n",
            "Starting epoch:  1149\n",
            "Avg Loss: 0.04143841937184334\n",
            "Starting epoch:  1150\n",
            "Avg Loss: 0.04122734069824219\n",
            "Score: 22.976903336184773\n",
            "Starting epoch:  1151\n",
            "Avg Loss: 0.04032309725880623\n",
            "Starting epoch:  1152\n",
            "Avg Loss: 0.0403437539935112\n",
            "Starting epoch:  1153\n",
            "Avg Loss: 0.04164827615022659\n",
            "Starting epoch:  1154\n",
            "Avg Loss: 0.04151856526732445\n",
            "Starting epoch:  1155\n",
            "Avg Loss: 0.04129703342914581\n",
            "Starting epoch:  1156\n",
            "Avg Loss: 0.04134565591812134\n",
            "Starting epoch:  1157\n",
            "Avg Loss: 0.04062449932098389\n",
            "Starting epoch:  1158\n",
            "Avg Loss: 0.0414336696267128\n",
            "Starting epoch:  1159\n",
            "Avg Loss: 0.04123018682003021\n",
            "Starting epoch:  1160\n",
            "Avg Loss: 0.0406511090695858\n",
            "Score: 22.93983461648132\n",
            "Starting epoch:  1161\n",
            "Avg Loss: 0.04134579747915268\n",
            "Starting epoch:  1162\n",
            "Avg Loss: 0.04128505289554596\n",
            "Starting epoch:  1163\n",
            "Avg Loss: 0.04102250561118126\n",
            "Starting epoch:  1164\n",
            "Avg Loss: 0.040689509361982346\n",
            "Starting epoch:  1165\n",
            "Avg Loss: 0.0409604087471962\n",
            "Starting epoch:  1166\n",
            "Avg Loss: 0.04076720029115677\n",
            "Starting epoch:  1167\n",
            "Avg Loss: 0.0414142869412899\n",
            "Starting epoch:  1168\n",
            "Avg Loss: 0.040545154362916946\n",
            "Starting epoch:  1169\n",
            "Avg Loss: 0.04127144813537598\n",
            "Starting epoch:  1170\n",
            "Avg Loss: 0.040958039462566376\n",
            "Score: 22.98545765611634\n",
            "Starting epoch:  1171\n",
            "Avg Loss: 0.04124566540122032\n",
            "Starting epoch:  1172\n",
            "Avg Loss: 0.04145914316177368\n",
            "Starting epoch:  1173\n",
            "Avg Loss: 0.04102868586778641\n",
            "Starting epoch:  1174\n",
            "Avg Loss: 0.040538035333156586\n",
            "Starting epoch:  1175\n",
            "Avg Loss: 0.04095626249909401\n",
            "Starting epoch:  1176\n",
            "Avg Loss: 0.0406462661921978\n",
            "Starting epoch:  1177\n",
            "Avg Loss: 0.040979597717523575\n",
            "Starting epoch:  1178\n",
            "Avg Loss: 0.04144071415066719\n",
            "Starting epoch:  1179\n",
            "Avg Loss: 0.04072543978691101\n",
            "Starting epoch:  1180\n",
            "Avg Loss: 0.040552716702222824\n",
            "Score: 22.976903336184773\n",
            "Starting epoch:  1181\n",
            "Avg Loss: 0.04183905944228172\n",
            "Starting epoch:  1182\n",
            "Avg Loss: 0.04175017401576042\n",
            "Starting epoch:  1183\n",
            "Avg Loss: 0.041186653077602386\n",
            "Starting epoch:  1184\n",
            "Avg Loss: 0.04089280217885971\n",
            "Starting epoch:  1185\n",
            "Avg Loss: 0.04088708758354187\n",
            "Starting epoch:  1186\n",
            "Avg Loss: 0.04104215279221535\n",
            "Starting epoch:  1187\n",
            "Avg Loss: 0.041172705590724945\n",
            "Starting epoch:  1188\n",
            "Avg Loss: 0.040631651878356934\n",
            "Starting epoch:  1189\n",
            "Avg Loss: 0.041155725717544556\n",
            "Starting epoch:  1190\n",
            "Avg Loss: 0.03992556780576706\n",
            "Score: 22.974051896207584\n",
            "Starting epoch:  1191\n",
            "Avg Loss: 0.04036657512187958\n",
            "Starting epoch:  1192\n",
            "Avg Loss: 0.03961460292339325\n",
            "Starting epoch:  1193\n",
            "Avg Loss: 0.04054830223321915\n",
            "Starting epoch:  1194\n",
            "Avg Loss: 0.0402861088514328\n",
            "Starting epoch:  1195\n",
            "Avg Loss: 0.04089200496673584\n",
            "Starting epoch:  1196\n",
            "Avg Loss: 0.04027997702360153\n",
            "Starting epoch:  1197\n",
            "Avg Loss: 0.04084451124072075\n",
            "Starting epoch:  1198\n",
            "Avg Loss: 0.04077576473355293\n",
            "Starting epoch:  1199\n",
            "Avg Loss: 0.040722113102674484\n",
            "Starting epoch:  1200\n",
            "Avg Loss: 0.04082246869802475\n",
            "Score: 23.031080695751356\n",
            "Starting epoch:  1201\n",
            "Avg Loss: 0.04098817706108093\n",
            "Starting epoch:  1202\n",
            "Avg Loss: 0.039831433445215225\n",
            "Starting epoch:  1203\n",
            "Avg Loss: 0.04017405956983566\n",
            "Starting epoch:  1204\n",
            "Avg Loss: 0.04064132645726204\n",
            "Starting epoch:  1205\n",
            "Avg Loss: 0.03983807936310768\n",
            "Starting epoch:  1206\n",
            "Avg Loss: 0.04001075401902199\n",
            "Starting epoch:  1207\n",
            "Avg Loss: 0.041554901748895645\n",
            "Starting epoch:  1208\n",
            "Avg Loss: 0.04038911312818527\n",
            "Starting epoch:  1209\n",
            "Avg Loss: 0.04027780890464783\n",
            "Starting epoch:  1210\n",
            "Avg Loss: 0.040939975529909134\n",
            "Score: 23.068149415454805\n",
            "Starting epoch:  1211\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-30e32672428a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnegative_triples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroken_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_relations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroken_tails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_triples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_triples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zero_grad_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "% matplotlib inline"
      ],
      "metadata": {
        "id": "BXNGo5SIF64h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "NqapXHs0GB0y",
        "outputId": "43bcdbc0-40d0-476e-9127-cbf92e12e3a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd03e2a81d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Uy6dQC5Lt-Fn",
        "outputId": "66b8317f-de25-496a-db9d-32be09901d1f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd03d210c90>]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the best checkpoint on test dataset\n",
        "storage.load_checkpoint(\"checkpoint.tar\", model, optimizer)\n",
        "best_model = model.to(device)\n",
        "best_model.eval()\n",
        "scores = test(model=best_model, data_generator=test_generator, entities_count=len(entity2id), device=device,\n",
        "              summary_writer=summary_writer, epoch_id=1, metric_suffix=\"test\")\n",
        "print(\"Test scores: \", scores)"
      ],
      "metadata": {
        "id": "Z0rOGvxonfeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8baf6b3c-27ca-4d4b-e65b-787200b5ff69"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test scores:  (7.583308902570117, 13.495553601094498, 22.798788234144435, 12.778492546100182)\n"
          ]
        }
      ]
    }
  ]
}